{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4be3e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db667cc",
   "metadata": {},
   "source": [
    "## Feature Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14adbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLab:\n",
    "    \"\"\"Shared mathematical engine for technical and statistical features.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_weights_frac_diff(d, size, threshold=1e-5):\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_k = -w[-1] / k * (d - k + 1)\n",
    "            w.append(w_k)\n",
    "        w = np.array(w[::-1])\n",
    "        w = w[np.abs(w) > threshold]\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def frac_diff_fixed(series, d, window=50):\n",
    "        # Solves Stationarity Dilemma [cite: 61]\n",
    "        weights = FeatureLab.get_weights_frac_diff(d, window)\n",
    "        res = series.rolling(window=len(weights)).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def yang_zhang_volatility(df, window=30):\n",
    "        # Captures intraday energy/gaps [cite: 82]\n",
    "        log_ho = (df['High'] / df['Open']).apply(np.log)\n",
    "        log_lo = (df['Low'] / df['Open']).apply(np.log)\n",
    "        log_co = (df['Close'] / df['Open']).apply(np.log)\n",
    "        log_oc = (df['Open'] / df['Close'].shift(1)).apply(np.log)\n",
    "        log_cc = (df['Close'] / df['Close'].shift(1)).apply(np.log)\n",
    "        \n",
    "        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
    "        close_vol = log_cc.rolling(window=window).var()\n",
    "        open_vol = log_oc.rolling(window=window).var()\n",
    "        window_rs = rs.rolling(window=window).mean()\n",
    "\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        return np.sqrt(open_vol + k * window_rs)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    @staticmethod\n",
    "    def triple_barrier_labels(prices, vol, pt=1.0, sl=1.0, barrier_window=10):\n",
    "        \"\"\"\n",
    "        Implements the Triple Barrier Method.\n",
    "        Labels: 1 (Profit Target Hit), -1 (Stop Loss Hit), 0 (Time Limit/Neutral)\n",
    "        \"\"\"\n",
    "        labels = pd.Series(0, index=prices.index)\n",
    "        # Shift prices to align future outcome with current row\n",
    "        # However, to avoid look-ahead in features, we usually compute label for row t based on t+1...t+k\n",
    "        # This function generates the TARGET variable (y) for training.\n",
    "        \n",
    "        limit = len(prices) - barrier_window\n",
    "        p_values = prices.values\n",
    "        v_values = vol.values\n",
    "        \n",
    "        for i in range(limit):\n",
    "            current_p = p_values[i]\n",
    "            current_vol = v_values[i]\n",
    "            \n",
    "            # Dynamic barriers based on volatility [cite: 215]\n",
    "            target = current_p * (1 + pt * current_vol)\n",
    "            stop = current_p * (1 - sl * current_vol)\n",
    "            \n",
    "            future_window = p_values[i+1 : i+1+barrier_window]\n",
    "            \n",
    "            hit_target = np.where(future_window >= target)[0]\n",
    "            hit_stop = np.where(future_window <= stop)[0]\n",
    "            \n",
    "            first_target = hit_target[0] if len(hit_target) > 0 else barrier_window + 1\n",
    "            first_stop = hit_stop[0] if len(hit_stop) > 0 else barrier_window + 1\n",
    "            \n",
    "            if first_target < first_stop and first_target <= barrier_window:\n",
    "                labels.iloc[i] = 1\n",
    "            elif first_stop < first_target and first_stop <= barrier_window:\n",
    "                labels.iloc[i] = 0 # In Meta-Labeling, we often treat Stop (-1) as 0 (Do Not Trade)\n",
    "            # Else 0 (Time limit reached or neutral)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381033d7",
   "metadata": {},
   "source": [
    "## Base Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5398e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseStrategy(ABC):\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.data = None\n",
    "        self.results = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        warmup_start_dt = start_dt - timedelta(days=warmup_years*365)\n",
    "        warmup_start_str = warmup_start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            self.data = yf.download(self.ticker, start=warmup_start_str, end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(self.data.columns, pd.MultiIndex): \n",
    "                self.data.columns = self.data.columns.get_level_values(0)\n",
    "            if 'Adj Close' not in self.data.columns: \n",
    "                self.data['Adj Close'] = self.data['Close']\n",
    "            self.data['Returns'] = self.data['Adj Close'].pct_change()\n",
    "            self.data.dropna(inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {self.ticker}: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_signals(self):\n",
    "        pass\n",
    "\n",
    "    def run_backtest(self, transaction_cost=0.0005, rebalance_threshold=0.1):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        \n",
    "        backtest_mask = self.data.index >= self.start_date\n",
    "        df = self.data.loc[backtest_mask].copy()\n",
    "        if df.empty: return\n",
    "\n",
    "        # Position Smoothing\n",
    "        clean_positions = []\n",
    "        current_pos = 0.0\n",
    "        raw_signals = df['Signal'].values\n",
    "        \n",
    "        for target in raw_signals:\n",
    "            if abs(target - current_pos) > rebalance_threshold:\n",
    "                current_pos = target\n",
    "            clean_positions.append(current_pos)\n",
    "            \n",
    "        df['Position'] = clean_positions\n",
    "        df['Prev_Position'] = df['Position'].shift(1).fillna(0)\n",
    "        df['Turnover'] = (df['Prev_Position'] - df['Position'].shift(2).fillna(0)).abs()\n",
    "        df['Gross_Returns'] = df['Prev_Position'] * df['Returns']\n",
    "        df['Net_Returns'] = df['Gross_Returns'] - (df['Turnover'] * transaction_cost)\n",
    "        df['Net_Returns'].fillna(0, inplace=True)\n",
    "        \n",
    "        df['Cumulative_Strategy'] = (1 + df['Net_Returns']).cumprod()\n",
    "        df['Cumulative_Market'] = (1 + df['Returns']).cumprod()\n",
    "        \n",
    "        roll_max = df['Cumulative_Strategy'].cummax()\n",
    "        df['Drawdown'] = (df['Cumulative_Strategy'] / roll_max) - 1.0\n",
    "        \n",
    "        self.results = df\n",
    "        \n",
    "        # Performance Calculation\n",
    "        total_ret = df['Cumulative_Strategy'].iloc[-1] - 1\n",
    "        vol = df['Net_Returns'].std() * np.sqrt(252)\n",
    "        sharpe = (df['Net_Returns'].mean() / df['Net_Returns'].std()) * np.sqrt(252) if vol > 0 else 0\n",
    "        max_dd = df['Drawdown'].min()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'Total Return': total_ret,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Max Drawdown': max_dd\n",
    "        }\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cee99",
   "metadata": {},
   "source": [
    "## Model V1-V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93a7c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV1_Baseline(BaseStrategy):\n",
    "    \"\"\"V1: Fixed FracDiff, Standard GMM.\"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "        df['Cluster'] = gmm.fit_predict(X_scaled)\n",
    "        \n",
    "        stats = df.groupby('Cluster')['Returns_Smoothed'].mean().sort_values().index\n",
    "        mapping = {stats[0]: -1, stats[1]: 0, stats[2]: 1}\n",
    "        df['Regime'] = df['Cluster'].map(mapping)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['Regime'] == 1) & (df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[(df['Regime'] == 0) & (df['RSI'] < 40), 'Signal'] = 1\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Vol_Scaler'] = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        df['Signal'] = df['Signal'] * df['Vol_Scaler']\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9445384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV3_Macro(BaseStrategy):\n",
    "    \"\"\"V3: Macro (SPY) Filter.\"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            self.spy_data = spy[['Adj Close']].rename(columns={'Adj Close': 'SPY_Price'})\n",
    "        except: pass\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left')\n",
    "            df['SPY_MA200'] = df['SPY_Price'].rolling(window=200).mean()\n",
    "            df['Macro_Bull'] = df['SPY_Price'] > df['SPY_MA200']\n",
    "        else:\n",
    "            df['Macro_Bull'] = True\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[df['Macro_Bull'] == False, 'Signal'] = 0\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Signal'] = df['Signal'] * (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e2ad3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV9_RegimeUnshackled(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V9 (Robust): The 'Unshackled' Regime Model with Walk-Forward Learning.\n",
    "    \n",
    "    CRITICAL FIX:\n",
    "    - Replaces static gmm.fit_predict() with a Rolling Walk-Forward loop.\n",
    "    - Eliminates Look-Ahead Bias by retraining the regime model every month\n",
    "      using only the trailing 1-year window.\n",
    "    - Solves the 'Label Switching' problem dynamically at each step.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            # Fetch Macro Context (SPY) for the 'Alpha Clause'\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            \n",
    "            # Simple Macro Trend (200 SMA)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        # [cite: 151] Kalman Filter for recursive state estimation\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.001, 0.1 # Static params for robustness\n",
    "        \n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            \n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "            \n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Feature Engineering ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: \n",
    "            df['Macro_Trend'] = 1\n",
    "            \n",
    "        # Volatility & Kalman Slope [cite: 82, 151]\n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        \n",
    "        # Features for GMM (Smoothed to reduce noise)\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        \n",
    "        # Drop NaN from warmup\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. Rolling Walk-Forward GMM ---\n",
    "        # We need to predict 'Regime_Type' for every row without looking ahead.\n",
    "        \n",
    "        train_window = 252  # Lookback: 1 Year\n",
    "        refit_step = 21     # Re-train every Month\n",
    "        \n",
    "        # Initialize default regime (CHOP)\n",
    "        df['Regime_Type'] = 'CHOP' \n",
    "        \n",
    "        # Prepare Feature Matrix\n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        indices = np.arange(len(df))\n",
    "        \n",
    "        # Start loop after the first training window\n",
    "        for t in range(train_window, len(df), refit_step):\n",
    "            start_idx = t - train_window\n",
    "            end_idx = t\n",
    "            predict_end_idx = min(t + refit_step, len(df))\n",
    "            \n",
    "            # A. Train on PAST window\n",
    "            X_train = X[start_idx:end_idx]\n",
    "            \n",
    "            # Scale locally (Standardization must also be walk-forward)\n",
    "            #  \"Standard algorithms assume training data... same probability distribution\"\n",
    "            scaler_local = StandardScaler()\n",
    "            X_train_scaled = scaler_local.fit_transform(X_train)\n",
    "            \n",
    "            try:\n",
    "                # Fit GMM\n",
    "                gmm = GaussianMixture(n_components=3, random_state=42, n_init=5)\n",
    "                gmm.fit(X_train_scaled)\n",
    "                \n",
    "                # B. Predict on NEXT window (Future)\n",
    "                X_future = X[end_idx:predict_end_idx]\n",
    "                X_future_scaled = scaler_local.transform(X_future)\n",
    "                clusters_future = gmm.predict(X_future_scaled)\n",
    "                \n",
    "                # C. Dynamic Label Mapping (Solve Label Switching)\n",
    "                # We map clusters to Bull/Bear based on the *Training* means\n",
    "                # Calculate mean return for each cluster center in the training set\n",
    "                # We can approximate this by predicting the training set again\n",
    "                train_clusters = gmm.predict(X_train_scaled)\n",
    "                \n",
    "                # Create a temp dataframe to sort clusters\n",
    "                temp_df = pd.DataFrame({\n",
    "                    'Ret': X_train[:, 0], # Returns is column 0\n",
    "                    'Cluster': train_clusters\n",
    "                })\n",
    "                \n",
    "                stats = temp_df.groupby('Cluster')['Ret'].mean().sort_values()\n",
    "                \n",
    "                # The cluster with lowest mean return = BEAR\n",
    "                # The cluster with highest mean return = BULL\n",
    "                # Middle = CHOP\n",
    "                if len(stats) == 3:\n",
    "                    bear_c = stats.index[0]\n",
    "                    chop_c = stats.index[1]\n",
    "                    bull_c = stats.index[2]\n",
    "                    \n",
    "                    mapping = {bear_c: 'BEAR', chop_c: 'CHOP', bull_c: 'BULL'}\n",
    "                else:\n",
    "                    # Fallback if GMM collapses to fewer clusters\n",
    "                    mapping = {c: 'CHOP' for c in stats.index}\n",
    "\n",
    "                # Apply mapping to the prediction window\n",
    "                regimes_mapped = [mapping.get(c, 'CHOP') for c in clusters_future]\n",
    "                \n",
    "                # Store results in the main DataFrame\n",
    "                # Use iloc for integer-based indexing on the slice\n",
    "                df.iloc[end_idx:predict_end_idx, df.columns.get_loc('Regime_Type')] = regimes_mapped\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Keep default 'CHOP' on failure\n",
    "                pass\n",
    "\n",
    "        # --- 3. Unshackled Signal Logic (unchanged from V9) ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # A. BULL REGIME: \"Trust The Trend\"\n",
    "        bull_signal = (df['Regime_Type'] == 'BULL')\n",
    "        df.loc[bull_signal, 'Signal'] = 1.0\n",
    "        \n",
    "        # Boost: If Kalman agrees (Strong Trend), increase leverage\n",
    "        strong_trend = bull_signal & (df['Kalman_Slope'] > 0)\n",
    "        df.loc[strong_trend, 'Signal'] = 1.3\n",
    "        \n",
    "        # B. CHOP REGIME: \"Buy Dips\"\n",
    "        # chop_buy = (df['Regime_Type'] == 'CHOP') & (df['RSI'] < 45)\n",
    "        # df.loc[chop_buy, 'Signal'] = 1.0\n",
    "\n",
    "        # 1. Calculate a \"Floor\" (e.g., Lower Bollinger Band)\n",
    "        df['SMA_20'] = df['Adj Close'].rolling(20).mean()\n",
    "        df['BB_Lower'] = df['SMA_20'] - 2 * df['SMA_20'].rolling(20).std()\n",
    "\n",
    "        # 2. Strict Chop Entry\n",
    "        # OLD: RSI < 45\n",
    "        # NEW: RSI < 45 AND Price > BB_Lower (Don't buy if it's crashing through the floor)\n",
    "        #      AND FracDiff > -0.1 (Don't buy if trend is catastrophically negative)\n",
    "\n",
    "        chop_buy = (\n",
    "            (df['Regime_Type'] == 'CHOP') & \n",
    "            (df['RSI'] < 45) & \n",
    "            (df['Adj Close'] > df['BB_Lower']) # The Falling Knife Filter\n",
    "        )\n",
    "        df.loc[chop_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # C. BEAR REGIME: \"Survival\" (Cash unless deep panic)\n",
    "        panic_buy = (df['Regime_Type'] == 'BEAR') & (df['RSI'] < 25)\n",
    "        df.loc[panic_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # --- 4. The Alpha Clause (Macro Handling) ---\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        \n",
    "        # Volatility Sizing (Risk Parity) [cite: 254]\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        # Macro Filter: Halve size if SPY is bearish, but don't exit fully (Alpha Clause)\n",
    "        macro_scaler = df['Macro_Trend'].map({1: 1.0, 0: 0.5})\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler * macro_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0729",
   "metadata": {},
   "source": [
    "## Q.L. Fang's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f311e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U0_MeanReversion(BaseStrategy):\n",
    "    \"\"\"\n",
    "    U0: Hourly-Granularity Mean Reversion.\n",
    "    Uses 'Wick-Free' Daily Highs/Lows derived from Hourly closes.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, sensitivity=20, period=14, k=2.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.sensitivity = sensitivity\n",
    "        self.period = period\n",
    "        self.k = k\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Clamp start date to Yahoo's 730-day hourly limit\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        limit_dt = datetime.now() - timedelta(days=725)\n",
    "        if start_dt < limit_dt: start_dt = limit_dt\n",
    "        \n",
    "        try:\n",
    "            # 1. Hourly Data\n",
    "            df_h = yf.download(self.ticker, interval=\"1h\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=True)\n",
    "            if df_h.empty: return\n",
    "            if isinstance(df_h.columns, pd.MultiIndex): df_h.columns = df_h.columns.get_level_values(0)\n",
    "            df_h = df_h[['Close']].dropna()\n",
    "            df_h.index = df_h.index.tz_localize(None)\n",
    "\n",
    "            # 2. Custom Daily Aggregation\n",
    "            daily_max = df_h['Close'].resample('1D').max()\n",
    "            daily_min = df_h['Close'].resample('1D').min()\n",
    "            daily_diff = daily_max - daily_min\n",
    "            \n",
    "            # 3. Standard Daily Data\n",
    "            df_d = yf.download(self.ticker, interval=\"1d\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=True)\n",
    "            if isinstance(df_d.columns, pd.MultiIndex): df_d.columns = df_d.columns.get_level_values(0)\n",
    "            \n",
    "            df_d['Returns'] = df_d['Close'].pct_change()\n",
    "            df_d['Daily_Max_H'] = daily_max\n",
    "            df_d['Daily_Min_H'] = daily_min\n",
    "            df_d['Daily_Diff_H'] = daily_diff\n",
    "            df_d.dropna(inplace=True)\n",
    "            self.data = df_d\n",
    "        except: self.data = pd.DataFrame()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # Features\n",
    "        df['ATR'] = df['Daily_Diff_H'].rolling(self.period).mean()\n",
    "        df['Roll_Max'] = df['Daily_Max_H'].rolling(self.sensitivity).max()\n",
    "        df['Roll_Min'] = df['Daily_Min_H'].rolling(self.sensitivity).min()\n",
    "        \n",
    "        # Shift (No Lookahead)\n",
    "        cols = ['ATR', 'Roll_Max', 'Roll_Min', 'Daily_Max_H', 'Daily_Min_H']\n",
    "        df[cols] = df[cols].shift(1)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Bands\n",
    "        df['Upper'] = df['Roll_Max'] + self.k * df['ATR']\n",
    "        df['Lower'] = df['Roll_Min'] - self.k * df['ATR']\n",
    "        \n",
    "        # State Machine Vectorization\n",
    "        pos = 0\n",
    "        signals = []\n",
    "        close = df['Close'].values\n",
    "        lower = df['Lower'].values\n",
    "        upper = df['Upper'].values\n",
    "        dmax = df['Daily_Max_H'].values\n",
    "        dmin = df['Daily_Min_H'].values\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if pos == 0:\n",
    "                if close[i] < lower[i]: pos = 1\n",
    "                elif close[i] > upper[i]: pos = -1\n",
    "            elif pos == 1:\n",
    "                if close[i] > dmax[i]: pos = 0\n",
    "            elif pos == -1:\n",
    "                if close[i] < dmin[i]: pos = 0\n",
    "            signals.append(pos)\n",
    "            \n",
    "        df['Signal'] = signals\n",
    "        # Normalize signal to same scale as V3/V9 (approx 0 to 1 leverage)\n",
    "        # U0 is aggressive, so we scale it down slightly\n",
    "        df['Signal'] = df['Signal'] * 0.7 \n",
    "        self.data = df\n",
    "\n",
    "# ==========================================\n",
    "# 3. HRP MATH ENGINE (The Allocator)\n",
    "# ==========================================\n",
    "class HRP_Allocator:\n",
    "    @staticmethod\n",
    "    def getIVP(cov):\n",
    "        ivp = 1. / np.diag(cov)\n",
    "        ivp /= ivp.sum()\n",
    "        return ivp\n",
    "\n",
    "    @staticmethod\n",
    "    def getClusterVar(cov, cItems):\n",
    "        cov_ = cov.loc[cItems, cItems] \n",
    "        w_ = HRP_Allocator.getIVP(cov_).reshape(-1, 1)\n",
    "        cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
    "        return cVar\n",
    "\n",
    "    @staticmethod\n",
    "    def getQuasiDiag(link):\n",
    "        link = link.astype(int)\n",
    "        sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "        numItems = link[-1, 3] \n",
    "        while sortIx.max() >= numItems:\n",
    "            sortIx.index = range(0, sortIx.shape[0] * 2, 2) \n",
    "            df0 = sortIx[sortIx >= numItems] \n",
    "            i = df0.index\n",
    "            j = df0.values - numItems\n",
    "            sortIx[i] = link[j, 0] \n",
    "            df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "            sortIx = sortIx.append(df0) \n",
    "            sortIx = sortIx.sort_index() \n",
    "            sortIx.index = range(sortIx.shape[0]) \n",
    "        return sortIx.tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def getRecBipart(cov, sortIx):\n",
    "        w = pd.Series(1, index=sortIx)\n",
    "        cItems = [sortIx] \n",
    "        while len(cItems) > 0:\n",
    "            cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "            for i in range(0, len(cItems), 2):\n",
    "                cItems0 = cItems[i] \n",
    "                cItems1 = cItems[i + 1] \n",
    "                cVar0 = HRP_Allocator.getClusterVar(cov, cItems0)\n",
    "                cVar1 = HRP_Allocator.getClusterVar(cov, cItems1)\n",
    "                alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "                w[cItems0] *= alpha \n",
    "                w[cItems1] *= 1 - alpha \n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize(returns_df):\n",
    "        # Handle constant/zero returns to avoid NaN correlations\n",
    "        if returns_df.std().min() < 1e-6:\n",
    "            return pd.Series(1/len(returns_df.columns), index=returns_df.columns)\n",
    "            \n",
    "        corr = returns_df.corr().fillna(0)\n",
    "        cov = returns_df.cov().fillna(0)\n",
    "        dist = ((1 - corr) / 2.) ** .5\n",
    "        link = sch.linkage(dist, 'single')\n",
    "        sortIx = HRP_Allocator.getQuasiDiag(link)\n",
    "        sortIx = corr.index[sortIx].tolist()\n",
    "        return HRP_Allocator.getRecBipart(cov, sortIx)\n",
    "\n",
    "class Strategy_Ensemble_HRP(BaseStrategy):\n",
    "    \"\"\"\n",
    "    Phase 1 Upgrade: The HRP Ensemble.\n",
    "    Allocates capital dynamically between V3 (Trend), V9 (Regime), and U0 (Mean Rev)\n",
    "    based on their CORRELATION structure, not just returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        self.u0 = U0_MeanReversion(ticker, start_date, end_date)\n",
    "    \n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        self.u0.fetch_data(warmup_years) # U0 has strictest data reqs\n",
    "        if self.u0.data is None or self.u0.data.empty: return\n",
    "        \n",
    "        # Sync V3/V9 to U0's available timeframe\n",
    "        valid_start = self.u0.data.index[0].strftime(\"%Y-%m-%d\")\n",
    "        self.v3.start_date = valid_start\n",
    "        self.v9.start_date = valid_start\n",
    "        \n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        self.data = self.u0.data.copy() # Use U0 as master index\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.u0.data is None: return\n",
    "        \n",
    "        # Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        self.u0.generate_signals()\n",
    "        \n",
    "        # Merge Signals\n",
    "        df = self.data.copy()\n",
    "        # Join by index to handle missing rows\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='inner')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='inner')\n",
    "        df = df.join(self.u0.data[['Signal']].rename(columns={'Signal':'S_U0'}), how='inner')\n",
    "        \n",
    "        # Simulate Returns for HRP\n",
    "        df['R_V3'] = df['S_V3'].shift(1) * df['Returns']\n",
    "        df['R_V9'] = df['S_V9'].shift(1) * df['Returns']\n",
    "        df['R_U0'] = df['S_U0'].shift(1) * df['Returns']\n",
    "        \n",
    "        # HRP Walk-Forward Loop\n",
    "        lookback = 63  # Short lookback for correlation (3 months)\n",
    "        rebal = 21     # Monthly rebalance\n",
    "        \n",
    "        df['W_V3'], df['W_V9'], df['W_U0'] = 0.33, 0.33, 0.33\n",
    "        \n",
    "        indices = df.index\n",
    "        for t in range(lookback, len(df), rebal):\n",
    "            start = indices[t-lookback]\n",
    "            end = indices[t]\n",
    "            test_end = indices[min(t+rebal, len(df)-1)]\n",
    "            \n",
    "            # Extract Returns History for Optimization\n",
    "            ret_hist = df.loc[start:end, ['R_V3', 'R_V9', 'R_U0']]\n",
    "            \n",
    "            # Run HRP\n",
    "            try:\n",
    "                weights = HRP_Allocator.optimize(ret_hist)\n",
    "                df.loc[end:test_end, 'W_V3'] = weights['R_V3']\n",
    "                df.loc[end:test_end, 'W_V9'] = weights['R_V9']\n",
    "                df.loc[end:test_end, 'W_U0'] = weights['R_U0']\n",
    "            except: pass\n",
    "            \n",
    "        # Final Ensemble Signal\n",
    "        df['Signal'] = (df['S_V3']*df['W_V3']) + (df['S_V9']*df['W_V9']) + (df['S_U0']*df['W_U0'])\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9598e",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "285c817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustBenchmark:\n",
    "    \"\"\"\n",
    "    Implements Walk-Forward Analysis and Deflated Sharpe Ratio logic.\n",
    "    Benchmarks multiple strategies without look-ahead bias[cite: 275].\n",
    "    \"\"\"\n",
    "    def __init__(self, tickers, start_date, end_date):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.results = []\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"{'STRATEGY':<10} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        strategies = {\n",
    "            \"V1_Base\": StrategyV1_Baseline,\n",
    "            # \"V2_GMM\": StrategyV2_Advanced,\n",
    "            \"V3_Macro\": StrategyV3_Macro,\n",
    "            # \"V4_Meta\": StrategyV4_Meta,\n",
    "            # \"V5_Kalman\": StrategyV5_KalmanState,\n",
    "            # \"V6_Inst\": StrategyV6_MetaLabeling,\n",
    "            \"V7_Optim\": StrategyV7_AdaptiveOptim,\n",
    "            \"V8_Final\": StrategyV8_GrandUnification,\n",
    "            \"V9_Unshack\": StrategyV9_RegimeUnshackled,\n",
    "            \"U0_MeanRev\": U0_MeanReversion\n",
    "        }\n",
    "\n",
    "        for ticker in self.tickers:\n",
    "            # Capture Buy & Hold first\n",
    "            bh = StrategyV1_Baseline(ticker, self.start_date, self.end_date)\n",
    "            bh.fetch_data()\n",
    "            bh.data['Signal'] = 1 # Force Buy\n",
    "            bh.run_backtest()\n",
    "            self._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "            \n",
    "            for name, StratClass in strategies.items():\n",
    "                try:\n",
    "                    strat = StratClass(ticker, self.start_date, self.end_date)\n",
    "                    strat.fetch_data(warmup_years=2)\n",
    "                    strat.generate_signals()\n",
    "                    strat.run_backtest()\n",
    "                    \n",
    "                    self._print_row(name, ticker, strat.metrics)\n",
    "                    \n",
    "                    # Store for portfolio level (optional)\n",
    "                    self.results.append({\n",
    "                        'Ticker': ticker,\n",
    "                        'Strategy': name,\n",
    "                        'Returns': strat.results['Net_Returns']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed {name} {ticker}: {e}\")\n",
    "            print(\"-\" * 75)\n",
    "\n",
    "    def _print_row(self, name, ticker, metrics):\n",
    "        if not metrics: return\n",
    "        ret = metrics['Total Return']\n",
    "        # Annualize return approx\n",
    "        ann_ret = (1 + ret) ** (252 / len(metrics.get('Returns', [1]*252))) - 1 if 'Returns' in metrics else ret\n",
    "        print(f\"{name:<10} | {ticker:<6} | {ret:.1%}   | {metrics['Sharpe Ratio']:.2f}   | {metrics['Max Drawdown']:.1%}   |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f62c7e",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80c0270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble(BaseStrategy):\n",
    "    \"\"\"\n",
    "    The 'All-Weather' Ensemble.\n",
    "    \n",
    "    Combines V3 (Macro Trend) and V9 (Regime Unshackled) into a single\n",
    "    portfolio-level signal.\n",
    "    \n",
    "    Logic:\n",
    "    1. Runs V3 to capture high-beta trends (NVDA, Bitcoin).\n",
    "    2. Runs V9 to capture regime-based alpha and protect downside (JPM, BABA).\n",
    "    3. Blends signals using a 'Correlation-Adjusted' weighting or fixed 50/50.\n",
    "    4. Applies a final Volatility Target to the combined equity curve to ensure\n",
    "       the two strategies don't stack up to dangerous leverage.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, w_v3=0.5, w_v9=0.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.w_v3 = w_v3\n",
    "        self.w_v9 = w_v9\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Fetch once for efficiency (logic could be optimized to share DF, \n",
    "        # but separate fetch ensures cleaner encapsulation)\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # We share the index/data from one of them for the main wrapper\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Align Indices (Inner Join to be safe)\n",
    "        df = self.data.copy()\n",
    "        s3 = self.strat_v3.data['Signal']\n",
    "        s9 = self.strat_v9.data['Signal']\n",
    "        \n",
    "        # Merge signals into main DF\n",
    "        df['Sig_V3'] = s3\n",
    "        df['Sig_V9'] = s9\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. The Allocation Logic\n",
    "        # Default: Fixed Weight (Core-Satellite Approach)\n",
    "        # V3 (Beta) + V9 (Alpha)\n",
    "        \n",
    "        # We blend the RAW signals.\n",
    "        # Note: Signals are already Vol-Targeted to ~15% inside sub-classes.\n",
    "        # Simple addition would double vol if correlation=1.\n",
    "        raw_blend = (df['Sig_V3'] * self.w_v3) + (df['Sig_V9'] * self.w_v9)\n",
    "        \n",
    "        # 3. Ensemble Volatility Control\n",
    "        # If V3 and V9 agree (both Long), we get high exposure.\n",
    "        # If they disagree (V3 Long, V9 Cash), we get half exposure.\n",
    "        # This naturally deleverages during uncertainty.\n",
    "        \n",
    "        df['Signal'] = raw_blend\n",
    "        \n",
    "        # Optional: Re-Target Volatility of the *Ensemble*\n",
    "        # (Prevents leverage creep if strategies are highly correlated)\n",
    "        # For now, we trust the weighted sum to act as a diversification benefit.\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd66f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble_Adaptive(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V10: The Adaptive Ensemble (Dynamic Weighting).\n",
    "    \n",
    "    Instead of fixed weights, this strategy re-allocates capital quarterly \n",
    "    based on the recent Risk-Adjusted Performance (Sharpe) of the sub-strategies.\n",
    "    \n",
    "    Logic:\n",
    "    1. Lookback: 126 Days (6 Months).\n",
    "    2. Rebalance: Every 63 Days (Quarterly).\n",
    "    3. Weighting:\n",
    "       - Calculate Sharpe Ratio for V3 and V9 in the lookback window.\n",
    "       - If Sharpe > 0: Weight is proportional to Sharpe.\n",
    "       - If Sharpe < 0: Weight is set to 0.\n",
    "       - Normalize weights to sum to 1.0.\n",
    "       \n",
    "    This allows the portfolio to automatically 'Risk On' into V3 during strong bulls\n",
    "    and 'Risk Off' into V9 during bears/chop.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # Use one of the dataframes as the base\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Merge Data\n",
    "        df = self.data.copy()\n",
    "        df['Sig_V3'] = self.strat_v3.data['Signal']\n",
    "        df['Sig_V9'] = self.strat_v9.data['Signal']\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. Simulate Sub-Strategy Returns (for metric calculation)\n",
    "        # We need to know how they *would* have performed to weight them.\n",
    "        # Lag signals by 1 to avoid lookahead bias when calculating returns.\n",
    "        df['Ret_V3'] = df['Sig_V3'].shift(1) * df['Returns']\n",
    "        df['Ret_V9'] = df['Sig_V9'].shift(1) * df['Returns']\n",
    "        \n",
    "        # 3. Walk-Forward Weight Optimization\n",
    "        df['W_V3'] = 0.5 # Default start\n",
    "        df['W_V9'] = 0.5\n",
    "        \n",
    "        lookback = 126      # 6 Months Lookback\n",
    "        rebalance_freq = 21 # Monthly Rebalance (Faster adaptation)\n",
    "        \n",
    "        indices = df.index\n",
    "        \n",
    "        if len(df) > lookback:\n",
    "            for t in range(lookback, len(df), rebalance_freq):\n",
    "                train_start = indices[t - lookback]\n",
    "                train_end = indices[t]\n",
    "                test_end_idx = min(t + rebalance_freq, len(df))\n",
    "                test_end = indices[test_end_idx - 1]\n",
    "                \n",
    "                # Calculate Sharpe in Lookback Window\n",
    "                # Add small epsilon to std to avoid division by zero\n",
    "                v3_mean = df.loc[train_start:train_end, 'Ret_V3'].mean()\n",
    "                v3_std = df.loc[train_start:train_end, 'Ret_V3'].std() + 1e-9\n",
    "                sharpe_v3 = (v3_mean / v3_std) * np.sqrt(252)\n",
    "                \n",
    "                v9_mean = df.loc[train_start:train_end, 'Ret_V9'].mean()\n",
    "                v9_std = df.loc[train_start:train_end, 'Ret_V9'].std() + 1e-9\n",
    "                sharpe_v9 = (v9_mean / v9_std) * np.sqrt(252)\n",
    "                \n",
    "                # Weighting Logic\n",
    "                # 1. Filter: If Sharpe is negative, set score to 0\n",
    "                score_v3 = max(0, sharpe_v3)\n",
    "                score_v9 = max(0, sharpe_v9)\n",
    "                \n",
    "                # 2. Normalize\n",
    "                total_score = score_v3 + score_v9\n",
    "                \n",
    "                if total_score > 0:\n",
    "                    w_v3 = score_v3 / total_score\n",
    "                    w_v9 = score_v9 / total_score\n",
    "                else:\n",
    "                    # Both are failing? Default to Defensive (V9) or Cash (0)\n",
    "                    # Let's default to V9 (Safety) as the 'bunker'\n",
    "                    w_v3 = 0.0\n",
    "                    w_v9 = 1.0\n",
    "                \n",
    "                # Apply weights to NEXT window\n",
    "                df.loc[train_end:test_end, 'W_V3'] = w_v3\n",
    "                df.loc[train_end:test_end, 'W_V9'] = w_v9\n",
    "                \n",
    "        # 4. Final Signal Generation\n",
    "        # Blend the signals using the dynamic weights\n",
    "        df['Signal'] = (df['Sig_V3'] * df['W_V3']) + (df['Sig_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5892e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV12_Macro_Switch(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V12: The Macro-Guided Ensemble.\n",
    "    \n",
    "    Replaces lookback windows with Real-Time Economic Data.\n",
    "    \n",
    "    DATA SOURCES (Yahoo Finance):\n",
    "    1. ^VIX: CBOE Volatility Index.\n",
    "    2. ^TNX: 10-Year Treasury Yield.\n",
    "    \n",
    "    LOGIC:\n",
    "    1. Calculate 'Macro Stress Score' (0.0 to 1.0).\n",
    "       - VIX Component: Normalized against recent history. High VIX = High Stress.\n",
    "       - Yield Component: Rate of Change (ROC) of TNX. Spiking rates = High Stress.\n",
    "    \n",
    "    2. Dynamic Weighting:\n",
    "       - Weight_V3 (Trend) = 1.0 - Stress_Score\n",
    "       - Weight_V9 (Safety) = Stress_Score\n",
    "       \n",
    "    HYPOTHESIS:\n",
    "    VIX and Rates often spike BEFORE the price crash is fully realized. \n",
    "    This allows the model to switch to safety faster than a Moving Average.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        # We store macro data separately\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # 1. Fetch Ticker Data\n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        \n",
    "        if self.v3.data is None or self.v3.data.empty: return\n",
    "        self.data = self.v3.data.copy()\n",
    "        \n",
    "        # 2. Fetch Macro Data (VIX and TNX)\n",
    "        start_dt = (self.data.index[0] - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "        end_dt = self.end_date\n",
    "        \n",
    "        try:\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            \n",
    "            # Cleaning\n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            macro_df = pd.DataFrame(index=self.data.index)\n",
    "            # Align macro data to the ticker's trading days (ffill for holidays)\n",
    "            macro_df['VIX'] = vix['Close'].reindex(self.data.index, method='ffill')\n",
    "            macro_df['TNX'] = tnx['Close'].reindex(self.data.index, method='ffill')\n",
    "            \n",
    "            self.macro_data = macro_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Macro Data Fetch Error: {e}\")\n",
    "            # Fallback: Zero stress\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        \n",
    "        # 1. Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        \n",
    "        # 2. Sync Data\n",
    "        df = self.data.copy()\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='left')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='left')\n",
    "        \n",
    "        # 3. Calculate Macro Stress Score\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # A. VIX Stress (Fear)\n",
    "        # Normalize VIX: If VIX > 30, Stress = 1.0. If VIX < 15, Stress = 0.0.\n",
    "        # Uses a rolling Z-score or simple clamp? Simple clamp is more robust to regime shifts.\n",
    "        macro['VIX_Stress'] = ((macro['VIX'] - 15) / (30 - 15)).clip(0, 1)\n",
    "        \n",
    "        # B. Yield Stress (Rate Shock)\n",
    "        # We care about SPEED of rate rise, not just level.\n",
    "        # Calculate 20-day Rate of Change of TNX\n",
    "        macro['TNX_ROC'] = macro['TNX'].pct_change(20)\n",
    "        # If Yields rise > 10% in a month, that's a shock.\n",
    "        macro['TNX_Stress'] = (macro['TNX_ROC'] / 0.10).clip(0, 1)\n",
    "        \n",
    "        # Combined Stress (Max of either Fear or Rate Shock)\n",
    "        # We use Max because either one can crash the market independently.\n",
    "        macro['Total_Stress'] = macro[['VIX_Stress', 'TNX_Stress']].max(axis=1)\n",
    "        \n",
    "        # 4. Allocate Weights\n",
    "        # Smooth the stress signal to avoid daily jitter (3-day avg)\n",
    "        stress_signal = macro['Total_Stress'].rolling(3).mean().fillna(0)\n",
    "        \n",
    "        df['W_V9'] = stress_signal        # High Stress -> More Safety\n",
    "        df['W_V3'] = 1.0 - stress_signal  # Low Stress -> More Trend\n",
    "        \n",
    "        # 5. Final Signal\n",
    "        df['Signal'] = (df['S_V3'] * df['W_V3']) + (df['S_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecba06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV14_AI_Selector(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V14: The AI 'Mixture of Experts'.\n",
    "    \n",
    "    Uses a Walk-Forward Random Forest to dynamically select the best sub-strategy.\n",
    "    \n",
    "    LOGIC:\n",
    "    1. Simulate Returns for V12 (Macro) and V9 (Regime).\n",
    "    2. Create Target Label:\n",
    "       - y = 1 if V12 outperforms V9 over the next 5 days.\n",
    "       - y = 0 if V9 outperforms V12.\n",
    "    3. Train Classifier:\n",
    "       - Features: VIX, TNX, Asset Volatility, RSI, Correlation.\n",
    "    4. Execution:\n",
    "       - If Model predicts 1 (High Confidence) -> Allocate to V12.\n",
    "       - If Model predicts 0 -> Allocate to V9.\n",
    "       \n",
    "    This removes the brittle '-0.2' threshold and allows the data to decide.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v12 = StrategyV12_Macro_Switch(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Fetch Sub-Strategies\n",
    "        self.v12.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        \n",
    "        if self.v12.data is None or self.v12.data.empty: return\n",
    "        self.data = self.v12.data.copy()\n",
    "        \n",
    "        # Extract Macro Data (for ML Features)\n",
    "        if self.v12.macro_data is not None:\n",
    "            self.macro_data = self.v12.macro_data.reindex(self.data.index).fillna(method='ffill')\n",
    "        else:\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        \n",
    "        # 1. Run Experts\n",
    "        self.v12.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        \n",
    "        # 2. Prepare Training Data\n",
    "        df = self.data.copy()\n",
    "        # Signals\n",
    "        df['S_V12'] = self.v12.data['Signal']\n",
    "        df['S_V9']  = self.v9.data['Signal']\n",
    "        \n",
    "        # Future Returns (Target)\n",
    "        # Which strategy performs better over next 5 days?\n",
    "        # We assume 1-day lag for implementation, so we look at t+1 to t+5\n",
    "        lookahead = 5\n",
    "        df['Ret_V12'] = (df['S_V12'].shift(1) * df['Returns']).rolling(lookahead).sum().shift(-lookahead)\n",
    "        df['Ret_V9']  = (df['S_V9'].shift(1) * df['Returns']).rolling(lookahead).sum().shift(-lookahead)\n",
    "        \n",
    "        # Target: 1 if V12 wins, 0 if V9 wins\n",
    "        df['Target'] = (df['Ret_V12'] > df['Ret_V9']).astype(int)\n",
    "        \n",
    "        # 3. Construct Features (X)\n",
    "        # Macro\n",
    "        df['VIX'] = self.macro_data['VIX']\n",
    "        df['TNX'] = self.macro_data['TNX']\n",
    "        df['VIX_Trend'] = df['VIX'].diff(5)\n",
    "        \n",
    "        # Micro\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Vol'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        \n",
    "        # Correlation (The V13 Feature)\n",
    "        df['Corr_VIX'] = df['Returns'].rolling(63).corr(self.macro_data['VIX'].pct_change())\n",
    "        \n",
    "        feature_cols = ['VIX', 'TNX', 'VIX_Trend', 'RSI', 'Vol', 'Corr_VIX']\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 4. Walk-Forward ML Loop\n",
    "        df['Prob_V12'] = 0.5 # Default Neutral\n",
    "        \n",
    "        train_window = 252 # 1 Year History to learn\n",
    "        rebal_freq = 21    # Retrain monthly\n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "        \n",
    "        indices = df.index\n",
    "        if len(df) > train_window:\n",
    "            for t in range(train_window, len(df), rebal_freq):\n",
    "                start = indices[t - train_window]\n",
    "                end = indices[t]\n",
    "                test_end = indices[min(t + rebal_freq, len(df) - 1)]\n",
    "                \n",
    "                # Train\n",
    "                X_train = df.loc[start:end, feature_cols]\n",
    "                y_train = df.loc[start:end, 'Target']\n",
    "                \n",
    "                try:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Predict\n",
    "                    X_test = df.loc[end:test_end, feature_cols]\n",
    "                    probs = clf.predict_proba(X_test)\n",
    "                    \n",
    "                    # Get Prob of Class 1 (V12 Wins)\n",
    "                    if probs.shape[1] == 2:\n",
    "                        p = probs[:, 1]\n",
    "                    else:\n",
    "                        p = probs[:, 0] if clf.classes_[0] == 1 else 0.0\n",
    "                        \n",
    "                    df.loc[end:test_end, 'Prob_V12'] = p\n",
    "                except: pass\n",
    "                \n",
    "        # 5. Signal Blending (Soft Voting)\n",
    "        # If Prob > 0.5, we lean to V12. If Prob < 0.5, we lean to V9.\n",
    "        # We map 0.0-1.0 probability to weights\n",
    "        \n",
    "        # Clip probability to avoid extreme overconfidence\n",
    "        prob = df['Prob_V12'].clip(0.1, 0.9)\n",
    "        \n",
    "        df['W_V12'] = prob\n",
    "        df['W_V9'] = 1.0 - prob\n",
    "        \n",
    "        df['Signal'] = (df['S_V12'] * df['W_V12']) + (df['S_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d2159",
   "metadata": {},
   "source": [
    "## Kalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bb62a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "# Assuming BaseStrategy and FeatureLab are already defined in your environment\n",
    "\n",
    "class StrategyV15_StatArb(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V15: Market Neutral Statistical Arbitrage (Native pykalman).\n",
    "    \n",
    "    Logic:\n",
    "    1. Model the relationship: Price_Y = Alpha + Beta * Price_X + Noise\n",
    "    2. Use Kalman Filter to estimate dynamic states [Alpha, Beta] in real-time.\n",
    "    3. Construct the Spread: Spread_t = Y_t - (Alpha_t + Beta_t * X_t).\n",
    "    4. Trade Mean Reversion of the Spread using Z-Scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker_y, ticker_x, start_date, end_date):\n",
    "        super().__init__(ticker_y, start_date, end_date)\n",
    "        self.ticker_x = ticker_x\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        start_str = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            # Download aligned data\n",
    "            data_y = yf.download(self.ticker, start=start_str, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            data_x = yf.download(self.ticker_x, start=start_str, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            \n",
    "            if isinstance(data_y.columns, pd.MultiIndex): data_y.columns = data_y.columns.get_level_values(0)\n",
    "            if isinstance(data_x.columns, pd.MultiIndex): data_x.columns = data_x.columns.get_level_values(0)\n",
    "            \n",
    "            df = pd.DataFrame(index=data_y.index)\n",
    "            df['Y'] = data_y['Close']\n",
    "            df['X'] = data_x['Close'].reindex(df.index).ffill()\n",
    "            df.dropna(inplace=True)\n",
    "            self.data = df\n",
    "        except Exception as e:\n",
    "            print(f\"StatArb Fetch Error: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Kalman Filter Setup ---\n",
    "        # State: [Alpha, Beta]\n",
    "        # Observation: Y ~ Alpha + Beta * X\n",
    "        \n",
    "        # Observation Matrices: Shape (N_samples, N_obs_dim, N_state_dim) -> (N, 1, 2)\n",
    "        # For each time t, Obs_Mat = [[1, X_t]]\n",
    "        obs_mat = np.expand_dims(np.vstack([[1] * len(df), df['X']]).T, axis=1)\n",
    "        \n",
    "        kf = KalmanFilter(\n",
    "            n_dim_obs=1, \n",
    "            n_dim_state=2, \n",
    "            initial_state_mean=[0, 0],\n",
    "            initial_state_covariance=np.ones((2, 2)),\n",
    "            transition_matrices=np.eye(2),       # Random Walk (Identity)\n",
    "            observation_matrices=obs_mat,        # Dynamic relationship\n",
    "            observation_covariance=1.0,          # Noise in price\n",
    "            transition_covariance=np.eye(2)*1e-4 # Noise in slope evolution (delta)\n",
    "        )\n",
    "        \n",
    "        # Run Filter (Causal, no lookahead)\n",
    "        # state_means contains [Alpha_t, Beta_t] for each step\n",
    "        state_means, _ = kf.filter(df['Y'].values)\n",
    "        \n",
    "        df['Alpha'] = state_means[:, 0]\n",
    "        df['Beta'] = state_means[:, 1]\n",
    "        \n",
    "        # --- 2. Spread Construction ---\n",
    "        # Spread = Actual Y - Predicted Y\n",
    "        df['Spread'] = df['Y'] - (df['Alpha'] + df['Beta'] * df['X'])\n",
    "        \n",
    "        # --- 3. Z-Score Logic ---\n",
    "        window = 20\n",
    "        df['Spread_Mean'] = df['Spread'].rolling(window).mean()\n",
    "        df['Spread_Std'] = df['Spread'].rolling(window).std()\n",
    "        df['Z_Score'] = (df['Spread'] - df['Spread_Mean']) / df['Spread_Std']\n",
    "        \n",
    "        # --- 4. Signal Generation (Mean Reversion) ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # Entry Logic\n",
    "        short_spread = (df['Z_Score'] > 2.0)  # Spread is too high -> Short Y, Long X\n",
    "        long_spread = (df['Z_Score'] < -2.0)  # Spread is too low -> Long Y, Short X\n",
    "        exit_cond = (df['Z_Score'].abs() < 0.5)\n",
    "        \n",
    "        # State Machine\n",
    "        curr_pos = 0 # 1 = Long Spread, -1 = Short Spread\n",
    "        pos_history = []\n",
    "        \n",
    "        # Vector -> Array for speed\n",
    "        ls_arr = long_spread.values\n",
    "        ss_arr = short_spread.values\n",
    "        ex_arr = exit_cond.values\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if curr_pos == 0:\n",
    "                if ls_arr[i]: curr_pos = 1\n",
    "                elif ss_arr[i]: curr_pos = -1\n",
    "            elif curr_pos == 1:\n",
    "                if ex_arr[i]: curr_pos = 0\n",
    "            elif curr_pos == -1:\n",
    "                if ex_arr[i]: curr_pos = 0\n",
    "            pos_history.append(curr_pos)\n",
    "            \n",
    "        # Shift position to avoid lookahead (Trade executed next open)\n",
    "        df['Spread_Position'] = pd.Series(pos_history, index=df.index).shift(1).fillna(0)\n",
    "        \n",
    "        # --- 5. Return Calculation ---\n",
    "        # Long Spread Profit = (Ret_Y) - Beta * (Ret_X)\n",
    "        df['Ret_Y'] = df['Y'].pct_change()\n",
    "        df['Ret_X'] = df['X'].pct_change()\n",
    "        \n",
    "        # We use the previous day's Hedge Ratio for the return calculation\n",
    "        df['Net_Returns'] = df['Spread_Position'] * (df['Ret_Y'] - df['Beta'].shift(1) * df['Ret_X'])\n",
    "        \n",
    "        # \"Signal\" column for compatibility with Portfolio Allocators\n",
    "        # Note: This signal is for PnL calculation only, logic is handled above\n",
    "        df['Signal'] = df['Spread_Position'] \n",
    "        \n",
    "        self.data = df\n",
    "\n",
    "    def run_backtest(self, transaction_cost=0.0005):\n",
    "        # Custom Backtest for Pairs (2x Transaction Costs)\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # Turnover: Change in position direction\n",
    "        df['Turnover'] = df['Spread_Position'].diff().abs().fillna(0)\n",
    "        \n",
    "        # Cost: 2 assets * cost\n",
    "        df['Strategy_Net'] = df['Net_Returns'] - (df['Turnover'] * transaction_cost * 2)\n",
    "        \n",
    "        df['Cumulative_Strategy'] = (1 + df['Strategy_Net']).cumprod()\n",
    "        roll_max = df['Cumulative_Strategy'].cummax()\n",
    "        df['Drawdown'] = (df['Cumulative_Strategy'] / roll_max) - 1.0\n",
    "        \n",
    "        self.results = df\n",
    "        \n",
    "        total_ret = df['Cumulative_Strategy'].iloc[-1] - 1\n",
    "        vol = df['Strategy_Net'].std() * np.sqrt(252)\n",
    "        sharpe = (df['Strategy_Net'].mean() / df['Strategy_Net'].std()) * np.sqrt(252) if vol > 0 else 0\n",
    "        max_dd = df['Drawdown'].min()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'Total Return': total_ret,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Max Drawdown': max_dd\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b85a9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "class StrategyV16_Gated_StatArb(StrategyV15_StatArb):\n",
    "    \"\"\"\n",
    "    V16: Cointegration-Gated Statistical Arbitrage.\n",
    "    \n",
    "    Improvements over V15:\n",
    "    1. STATIONARITY FILTER:\n",
    "       - Before taking a trade, we calculate the Rolling ADF P-Value of the spread.\n",
    "       - If P-Value > 0.05 (Non-Stationary), we set Signal to 0.\n",
    "       - This prevents the model from fighting a structural trend (e.g., NVDA vs AMD).\n",
    "       \n",
    "    2. HARD STOP LOSS:\n",
    "       - If Z-Score > 4.0, we assume the relationship has broken permanently ('Divorce').\n",
    "       - We force exit to prevent infinite drawdowns.\n",
    "    \"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        \n",
    "        # 1. Run Standard Kalman Filter (Inherited from V15)\n",
    "        # This populates self.data with 'Spread', 'Z_Score', 'Hedge_Ratio'\n",
    "        super().generate_signals()\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # 2. Rolling ADF Test (The Gatekeeper)\n",
    "        # We need a window (e.g., 60 days) to test for mean reversion\n",
    "        window = 60\n",
    "        adf_pvalues = []\n",
    "        spread_values = df['Spread'].values\n",
    "        \n",
    "        # This loop can be slow, but necessary for Walk-Forward validity\n",
    "        for i in range(len(df)):\n",
    "            if i < window:\n",
    "                adf_pvalues.append(1.0) # Assume non-stationary (safe) at start\n",
    "            else:\n",
    "                # Check stationarity of the recent spread history\n",
    "                window_spread = spread_values[i-window:i]\n",
    "                # Check if spread is constant (to avoid adfuller error)\n",
    "                if np.std(window_spread) < 1e-6:\n",
    "                    p_val = 1.0\n",
    "                else:\n",
    "                    try:\n",
    "                        # regression='c' (constant mean) or 'ct' (trend)\n",
    "                        res = adfuller(window_spread, regression='c')\n",
    "                        p_val = res[1]\n",
    "                    except:\n",
    "                        p_val = 1.0\n",
    "                adf_pvalues.append(p_val)\n",
    "                \n",
    "        df['ADF_PValue'] = adf_pvalues\n",
    "        \n",
    "        # 3. Apply The Filter\n",
    "        # Mask: 1 if Stationary (Trade), 0 if Trending (Don't Trade)\n",
    "        # We use a lag to avoid lookahead bias\n",
    "        is_stationary = (df['ADF_PValue'].shift(1) < 0.05).astype(int)\n",
    "        \n",
    "        # 4. Apply Hard Stop Loss (The \"Divorce\" Clause)\n",
    "        # If Spread Z-Score is > 4 sigma, the pair is likely broken.\n",
    "        is_broken = (df['Z_Score'].abs().shift(1) > 4.0).astype(int)\n",
    "        \n",
    "        # 5. Final Signal Logic\n",
    "        # Original Signal (from V15) * Stationarity Mask * (Not Broken)\n",
    "        # Note: 'Signal' in V15 contains the position (-1, 0, 1)\n",
    "        \n",
    "        original_signal = df['Spread_Position'] # This was the calculated position in V15\n",
    "        \n",
    "        # If not stationary, force position to 0\n",
    "        filtered_signal = original_signal * is_stationary\n",
    "        \n",
    "        # If broken, force position to 0\n",
    "        filtered_signal = np.where(is_broken == 1, 0, filtered_signal)\n",
    "        \n",
    "        df['Spread_Position'] = filtered_signal\n",
    "        \n",
    "        # Recalculate Returns with filtered positions\n",
    "        df['Net_Returns'] = df['Spread_Position'] * (df['Ret_Y'] - df['Beta'].shift(1) * df['Ret_X'])\n",
    "        \n",
    "        # Update Data\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6ae43",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac5dc656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATEGY     | TICKER | ANN RET | SHARPE | MAX DD  | NOTES\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | NVDA   | 355.4%   | 1.19   | -62.7%   |\n",
      "V3_Macro   | NVDA   | 173.4%   | 1.51   | -23.1%   |\n",
      "V9_Unshack | NVDA   | 61.9%   | 0.88   | -16.7%   |\n",
      "Ens_Adapt  | NVDA   | 111.8%   | 1.31   | -20.6%   |\n",
      "V12_Macro  | NVDA   | 159.6%   | 1.57   | -16.7%   |\n",
      "V14_AI     | NVDA   | 91.8%   | 1.19   | -16.7%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | JPM    | 62.1%   | 0.77   | -37.9%   |\n",
      "V3_Macro   | JPM    | 71.9%   | 0.97   | -28.5%   |\n",
      "V9_Unshack | JPM    | 39.2%   | 0.70   | -16.7%   |\n",
      "Ens_Adapt  | JPM    | 80.4%   | 1.16   | -17.9%   |\n",
      "V12_Macro  | JPM    | 93.1%   | 1.25   | -13.7%   |\n",
      "V14_AI     | JPM    | 55.7%   | 0.93   | -15.9%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | TSLA   | 7.9%   | 0.35   | -73.0%   |\n",
      "V3_Macro   | TSLA   | 31.4%   | 0.54   | -31.4%   |\n",
      "V9_Unshack | TSLA   | 49.6%   | 0.86   | -18.7%   |\n",
      "Ens_Adapt  | TSLA   | 23.9%   | 0.47   | -21.7%   |\n",
      "V12_Macro  | TSLA   | 30.8%   | 0.58   | -25.0%   |\n",
      "V14_AI     | TSLA   | 41.7%   | 0.77   | -20.3%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | BABA   | -26.9%   | 0.06   | -54.0%   |\n",
      "V3_Macro   | BABA   | 0.6%   | 0.10   | -25.3%   |\n",
      "V9_Unshack | BABA   | 1.8%   | 0.11   | -18.5%   |\n",
      "Ens_Adapt  | BABA   | 6.3%   | 0.22   | -18.5%   |\n",
      "V12_Macro  | BABA   | 0.4%   | 0.08   | -22.0%   |\n",
      "V14_AI     | BABA   | 6.9%   | 0.25   | -17.4%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | XLE    | 65.0%   | 0.77   | -26.0%   |\n",
      "V3_Macro   | XLE    | 17.2%   | 0.38   | -21.8%   |\n",
      "V9_Unshack | XLE    | 1.9%   | 0.12   | -22.0%   |\n",
      "Ens_Adapt  | XLE    | -4.5%   | -0.02   | -26.3%   |\n",
      "V12_Macro  | XLE    | 4.5%   | 0.17   | -24.4%   |\n",
      "V14_AI     | XLE    | -0.8%   | 0.06   | -20.1%   |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"{'STRATEGY':<12} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "    print(\"-\" * 79)\n",
    "    \n",
    "    # Helper wrappers for the dictionary\n",
    "    class Strategy_Ensemble_5050(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.5, 0.5)\n",
    "\n",
    "    class Strategy_Ensemble_Growth(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.7, 0.3)\n",
    "\n",
    "    strategies = {\n",
    "        \"V3_Macro\": StrategyV3_Macro,\n",
    "        \"V9_Unshack\": StrategyV9_RegimeUnshackled,\n",
    "        # \"Ens_Bal\": Strategy_Ensemble_5050,      # Static 50/50\n",
    "        # \"Ens_Grow\": Strategy_Ensemble_Growth,   # Static 70/30\n",
    "        \"Ens_Adapt\": Strategy_Ensemble_Adaptive, # Dynamic V10\n",
    "        # \"HRP_Base\": Strategy_Ensemble_HRP,\n",
    "        \"V12_Macro\": StrategyV12_Macro_Switch,\n",
    "        \"V14_AI\": StrategyV14_AI_Selector,\n",
    "    }\n",
    "\n",
    "    # Same Stress Test Basket\n",
    "    tickers = [\"NVDA\", \"JPM\", \"TSLA\", \"BABA\", \"XLE\"]\n",
    "\n",
    "    bench = RobustBenchmark(\n",
    "        tickers=tickers, \n",
    "        start_date=\"2022-01-01\", \n",
    "        end_date=\"2024-12-30\"\n",
    "    )\n",
    "    \n",
    "    # Manual run loop to handle the specific classes\n",
    "    for ticker in tickers:\n",
    "        # Buy & Hold\n",
    "        bh = StrategyV1_Baseline(ticker, bench.start_date, bench.end_date)\n",
    "        bh.fetch_data()\n",
    "        bh.data['Signal'] = 1\n",
    "        bh.run_backtest()\n",
    "        bench._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "        \n",
    "        for name, StratClass in strategies.items():\n",
    "            try:\n",
    "                strat = StratClass(ticker, bench.start_date, bench.end_date)\n",
    "                strat.fetch_data(warmup_years=2)\n",
    "                strat.generate_signals()\n",
    "                strat.run_backtest()\n",
    "                bench._print_row(name, ticker, strat.metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Err {name} {ticker}: {e}\")\n",
    "        print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6874e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATEGY     | PAIR      | ANN RET | SHARPE | MAX DD  | NOTES\n",
      "-------------------------------------------------------------------------------------\n",
      "V15_StatArb  | NVDA/AMD   | -11.0%   | -0.67   | -55.9%   | Raw Kalman\n",
      "V16_Gated    | NVDA/AMD   | -8.9%   | -0.55   | -52.2%   | Stationarity Check\n",
      "-------------------------------------------------------------------------------------\n",
      "V15_StatArb  | PEP/KO    | -9.8%   | -0.87   | -35.5%   | Raw Kalman\n",
      "V16_Gated    | PEP/KO    | -5.4%   | -0.53   | -21.6%   | Stationarity Check\n",
      "-------------------------------------------------------------------------------------\n",
      "V15_StatArb  | JPM/BAC   | -7.7%   | 0.03   | -64.8%   | Raw Kalman\n",
      "V16_Gated    | JPM/BAC   | 33.2%   | 1.09   | -30.2%   | Stationarity Check\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"{'STRATEGY':<12} | {'PAIR':<9} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    pairs = [\n",
    "        (\"NVDA\", \"AMD\"),  # The Broken Pair\n",
    "        (\"PEP\", \"KO\"),    # The Stable Pair\n",
    "        (\"JPM\", \"BAC\")    # The Correlated Pair\n",
    "    ]\n",
    "    \n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2024-12-30\"\n",
    "    \n",
    "    strategies = {\n",
    "        \"V15_StatArb\": StrategyV15_StatArb,\n",
    "        \"V16_Gated\": StrategyV16_Gated_StatArb\n",
    "    }\n",
    "    \n",
    "    for y, x in pairs:\n",
    "        for name, StratClass in strategies.items():\n",
    "            try:\n",
    "                strat = StratClass(y, x, start_date, end_date)\n",
    "                strat.fetch_data()\n",
    "                strat.generate_signals()\n",
    "                strat.run_backtest()\n",
    "                \n",
    "                m = strat.metrics\n",
    "                if m:\n",
    "                    ann_ret = (1 + m['Total Return'])**(252/len(strat.results)) - 1\n",
    "                    print(f\"{name:<12} | {y}/{x:<5} | {ann_ret:.1%}   | {m['Sharpe Ratio']:.2f}   | {m['Max Drawdown']:.1%}   | {'Stationarity Check' if 'Gated' in name else 'Raw Kalman'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error {name} {y}/{x}: {e}\")\n",
    "        print(\"-\" * 85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
