{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4be3e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db667cc",
   "metadata": {},
   "source": [
    "## 1. FEATURE ENGINEERING LAB (The Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14adbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLab:\n",
    "    \"\"\"Shared mathematical engine for technical and statistical features.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_weights_frac_diff(d, size, threshold=1e-5):\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_k = -w[-1] / k * (d - k + 1)\n",
    "            w.append(w_k)\n",
    "        w = np.array(w[::-1])\n",
    "        w = w[np.abs(w) > threshold]\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def frac_diff_fixed(series, d, window=50):\n",
    "        # Solves Stationarity Dilemma [cite: 61]\n",
    "        weights = FeatureLab.get_weights_frac_diff(d, window)\n",
    "        res = series.rolling(window=len(weights)).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def yang_zhang_volatility(df, window=30):\n",
    "        # Captures intraday energy/gaps [cite: 82]\n",
    "        log_ho = (df['High'] / df['Open']).apply(np.log)\n",
    "        log_lo = (df['Low'] / df['Open']).apply(np.log)\n",
    "        log_co = (df['Close'] / df['Open']).apply(np.log)\n",
    "        log_oc = (df['Open'] / df['Close'].shift(1)).apply(np.log)\n",
    "        log_cc = (df['Close'] / df['Close'].shift(1)).apply(np.log)\n",
    "        \n",
    "        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
    "        close_vol = log_cc.rolling(window=window).var()\n",
    "        open_vol = log_oc.rolling(window=window).var()\n",
    "        window_rs = rs.rolling(window=window).mean()\n",
    "\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        return np.sqrt(open_vol + k * window_rs)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    @staticmethod\n",
    "    def triple_barrier_labels(prices, vol, pt=1.0, sl=1.0, barrier_window=10):\n",
    "        \"\"\"\n",
    "        Implements the Triple Barrier Method.\n",
    "        Labels: 1 (Profit Target Hit), -1 (Stop Loss Hit), 0 (Time Limit/Neutral)\n",
    "        \"\"\"\n",
    "        labels = pd.Series(0, index=prices.index)\n",
    "        # Shift prices to align future outcome with current row\n",
    "        # However, to avoid look-ahead in features, we usually compute label for row t based on t+1...t+k\n",
    "        # This function generates the TARGET variable (y) for training.\n",
    "        \n",
    "        limit = len(prices) - barrier_window\n",
    "        p_values = prices.values\n",
    "        v_values = vol.values\n",
    "        \n",
    "        for i in range(limit):\n",
    "            current_p = p_values[i]\n",
    "            current_vol = v_values[i]\n",
    "            \n",
    "            # Dynamic barriers based on volatility [cite: 215]\n",
    "            target = current_p * (1 + pt * current_vol)\n",
    "            stop = current_p * (1 - sl * current_vol)\n",
    "            \n",
    "            future_window = p_values[i+1 : i+1+barrier_window]\n",
    "            \n",
    "            hit_target = np.where(future_window >= target)[0]\n",
    "            hit_stop = np.where(future_window <= stop)[0]\n",
    "            \n",
    "            first_target = hit_target[0] if len(hit_target) > 0 else barrier_window + 1\n",
    "            first_stop = hit_stop[0] if len(hit_stop) > 0 else barrier_window + 1\n",
    "            \n",
    "            if first_target < first_stop and first_target <= barrier_window:\n",
    "                labels.iloc[i] = 1\n",
    "            elif first_stop < first_target and first_stop <= barrier_window:\n",
    "                labels.iloc[i] = 0 # In Meta-Labeling, we often treat Stop (-1) as 0 (Do Not Trade)\n",
    "            # Else 0 (Time limit reached or neutral)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40668a35",
   "metadata": {},
   "source": [
    "## 2. BASE STRATEGY INFRASTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5398e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseStrategy(ABC):\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.data = None\n",
    "        self.results = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        warmup_start_dt = start_dt - timedelta(days=warmup_years*365)\n",
    "        warmup_start_str = warmup_start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            self.data = yf.download(self.ticker, start=warmup_start_str, end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(self.data.columns, pd.MultiIndex): \n",
    "                self.data.columns = self.data.columns.get_level_values(0)\n",
    "            if 'Adj Close' not in self.data.columns: \n",
    "                self.data['Adj Close'] = self.data['Close']\n",
    "            self.data['Returns'] = self.data['Adj Close'].pct_change()\n",
    "            self.data.dropna(inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {self.ticker}: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_signals(self):\n",
    "        pass\n",
    "\n",
    "    def run_backtest(self, transaction_cost=0.0005, rebalance_threshold=0.1):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        \n",
    "        backtest_mask = self.data.index >= self.start_date\n",
    "        df = self.data.loc[backtest_mask].copy()\n",
    "        if df.empty: return\n",
    "\n",
    "        # Position Smoothing\n",
    "        clean_positions = []\n",
    "        current_pos = 0.0\n",
    "        raw_signals = df['Signal'].values\n",
    "        \n",
    "        for target in raw_signals:\n",
    "            if abs(target - current_pos) > rebalance_threshold:\n",
    "                current_pos = target\n",
    "            clean_positions.append(current_pos)\n",
    "            \n",
    "        df['Position'] = clean_positions\n",
    "        df['Prev_Position'] = df['Position'].shift(1).fillna(0)\n",
    "        df['Turnover'] = (df['Prev_Position'] - df['Position'].shift(2).fillna(0)).abs()\n",
    "        df['Gross_Returns'] = df['Prev_Position'] * df['Returns']\n",
    "        df['Net_Returns'] = df['Gross_Returns'] - (df['Turnover'] * transaction_cost)\n",
    "        df['Net_Returns'].fillna(0, inplace=True)\n",
    "        \n",
    "        df['Cumulative_Strategy'] = (1 + df['Net_Returns']).cumprod()\n",
    "        df['Cumulative_Market'] = (1 + df['Returns']).cumprod()\n",
    "        \n",
    "        roll_max = df['Cumulative_Strategy'].cummax()\n",
    "        df['Drawdown'] = (df['Cumulative_Strategy'] / roll_max) - 1.0\n",
    "        \n",
    "        self.results = df\n",
    "        \n",
    "        # Performance Calculation\n",
    "        total_ret = df['Cumulative_Strategy'].iloc[-1] - 1\n",
    "        vol = df['Net_Returns'].std() * np.sqrt(252)\n",
    "        sharpe = (df['Net_Returns'].mean() / df['Net_Returns'].std()) * np.sqrt(252) if vol > 0 else 0\n",
    "        max_dd = df['Drawdown'].min()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'Total Return': total_ret,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Max Drawdown': max_dd\n",
    "        }\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cee99",
   "metadata": {},
   "source": [
    "## 3.  MODELS (V1-V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36fd0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV1_Baseline(BaseStrategy):\n",
    "    \"\"\"V1: Fixed FracDiff, Standard GMM.\"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "        df['Cluster'] = gmm.fit_predict(X_scaled)\n",
    "        \n",
    "        stats = df.groupby('Cluster')['Returns_Smoothed'].mean().sort_values().index\n",
    "        mapping = {stats[0]: -1, stats[1]: 0, stats[2]: 1}\n",
    "        df['Regime'] = df['Cluster'].map(mapping)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['Regime'] == 1) & (df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[(df['Regime'] == 0) & (df['RSI'] < 40), 'Signal'] = 1\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Vol_Scaler'] = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        df['Signal'] = df['Signal'] * df['Vol_Scaler']\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d053d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV2_Advanced(BaseStrategy):\n",
    "    \"\"\"V2: Rolling GMM.\"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        df['Regime'] = 0\n",
    "        window_size, step_size = 504, 126\n",
    "        preds, indices = [], []\n",
    "        \n",
    "        if len(df) > window_size:\n",
    "            for t in range(window_size, len(df), step_size):\n",
    "                train = df.iloc[t-window_size:t]\n",
    "                test = df.iloc[t:t+step_size]\n",
    "                if test.empty: break\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_s = scaler.fit_transform(train[['Returns_Smoothed', 'Vol_Smoothed']].values)\n",
    "                X_test_s = scaler.transform(test[['Returns_Smoothed', 'Vol_Smoothed']].values)\n",
    "                \n",
    "                gmm = GaussianMixture(n_components=3, random_state=42).fit(X_train_s)\n",
    "                train['Clust'] = gmm.predict(X_train_s)\n",
    "                stats = train.groupby('Clust')['Returns_Smoothed'].mean().sort_values().index\n",
    "                mapping = {stats[0]: -1, stats[1]: 0, stats[2]: 1}\n",
    "                \n",
    "                preds.extend([mapping[x] for x in gmm.predict(X_test_s)])\n",
    "                indices.extend(test.index)\n",
    "            \n",
    "            df.loc[indices, 'Regime'] = pd.Series(preds, index=indices)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['Regime'] == 1) & (df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[(df['Regime'] == 0) & (df['RSI'] < 45), 'Signal'] = 1\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Vol_Scaler'] = (target_vol / df['Volatility']).clip(upper=1.0)\n",
    "        df['Signal'] = df['Signal'] * df['Vol_Scaler']\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9445384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV3_Macro(BaseStrategy):\n",
    "    \"\"\"V3: Macro (SPY) Filter.\"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            self.spy_data = spy[['Adj Close']].rename(columns={'Adj Close': 'SPY_Price'})\n",
    "        except: pass\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left')\n",
    "            df['SPY_MA200'] = df['SPY_Price'].rolling(window=200).mean()\n",
    "            df['Macro_Bull'] = df['SPY_Price'] > df['SPY_MA200']\n",
    "        else:\n",
    "            df['Macro_Bull'] = True\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[df['Macro_Bull'] == False, 'Signal'] = 0\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Signal'] = df['Signal'] * (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2049efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV4_Meta(BaseStrategy):\n",
    "    \"\"\"V4: Dynamic Profiling with OBV.\"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
    "        df['OBV_Trend'] = df['OBV'].rolling(50).mean()\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        # Trend\n",
    "        df.loc[(df['FracDiff'] > 0) & (df['OBV'] > df['OBV_Trend']), 'Signal'] = 1\n",
    "        # Reversion\n",
    "        df.loc[(df['RSI'] < 30), 'Signal'] = 1\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Signal'] = df['Signal'] * (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fa87be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV5_KalmanState(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V5 (Formerly V10): Kalman Filter + Macro Filter + Volatility Burst Control.\n",
    "    Uses Kalman Filter for noise-free slope estimation[cite: 151].\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.001, 0.1\n",
    "        \n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else:\n",
    "            df['Macro_Trend'] = 1 \n",
    "            \n",
    "        log_prices = np.log(df['Adj Close'])\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(log_prices)\n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Vol_Change'] = df['Volatility'].diff()\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Primary Logic\n",
    "        df['Signal'] = 0.0\n",
    "        long_condition = (df['Kalman_Slope'] > 0) & (df['Macro_Trend'] == 1)\n",
    "        df.loc[long_condition, 'Signal'] = 1\n",
    "        \n",
    "        # Vol Targeting & Burst Protection\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Vol_Scaler'] = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        vol_spike = df['Vol_Change'] > df['Vol_Change'].rolling(20).std() * 2\n",
    "        df.loc[vol_spike, 'Vol_Scaler'] *= 0.5\n",
    "        df.loc[df['Macro_Trend'] == 0, 'Vol_Scaler'] *= 0.5\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * df['Vol_Scaler']\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ae392",
   "metadata": {},
   "source": [
    "## 4. NEW MODEL: STRATEGY V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd4e1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV6_MetaLabeling(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V6.1 (Hybrid): The 'Regime-Adaptive' Institutional Model.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Primary Signal (Hybrid): \n",
    "       - TREND: Kalman Slope > 0 (Catch the run)\n",
    "       - VALUE: RSI < 30 (Catch the dip)\n",
    "       This ensures we have candidates in both trending and chopping markets.\n",
    "       \n",
    "    2. Meta-Labeling (Random Forest): \n",
    "       - Learns WHICH of the above signals works for the current asset/regime.\n",
    "       \n",
    "    3. Soft-Sizing: Scales leverage based on ML confidence.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        # Kalman Params\n",
    "        P, Q, R = 1.0, 0.001, 0.1 \n",
    "        \n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            \n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "            \n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Features ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: df['Macro_Trend'] = 1\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Spread'] = df['Adj Close'] - df['Adj Close'].rolling(20).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. Primary Signal (The Hybrid Generator) ---\n",
    "        df['Primary_Signal'] = 0\n",
    "        \n",
    "        # A. MOMENTUM LEG (For NVDA/SPY)\n",
    "        # Catch the trend when slope is positive\n",
    "        trend_signal = (df['Kalman_Slope'] > 0)\n",
    "        \n",
    "        # B. MEAN REVERSION LEG (For JPM/Chop)\n",
    "        # Catch the knife when oversold (Value)\n",
    "        value_signal = (df['RSI'] < 30)\n",
    "        \n",
    "        # Combine: We are interested if EITHER is true\n",
    "        df.loc[trend_signal | value_signal, 'Primary_Signal'] = 1\n",
    "        \n",
    "        # --- 3. Meta-Labeling (The Validator) ---\n",
    "        # Label: Did buying here result in profit?\n",
    "        labels = FeatureLab.triple_barrier_labels(df['Adj Close'], df['Volatility'], pt=1.0, sl=1.0, barrier_window=10)\n",
    "        \n",
    "        df['Meta_Prob'] = 0.5\n",
    "        train_window = 252 * 2\n",
    "        update_freq = 63 \n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "        feature_cols = ['Volatility', 'RSI', 'Spread', 'Kalman_Slope']\n",
    "        \n",
    "        indices = df.index\n",
    "        if len(df) > train_window:\n",
    "            for t in range(train_window, len(df), update_freq):\n",
    "                train_start = indices[t - train_window]\n",
    "                train_end = indices[t]\n",
    "                test_end_idx = min(t + update_freq, len(df))\n",
    "                test_end = indices[test_end_idx - 1]\n",
    "                \n",
    "                X_train = df.loc[train_start:train_end, feature_cols]\n",
    "                y_train = labels.loc[train_start:train_end]\n",
    "                \n",
    "                # Training on all data allows the model to learn \"High RSI = Good\" for NVDA\n",
    "                # and \"Low RSI = Good\" for JPM automatically based on recent history.\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                X_test = df.loc[train_end:test_end, feature_cols]\n",
    "                probs = clf.predict_proba(X_test)\n",
    "                \n",
    "                if probs.shape[1] == 2:\n",
    "                    pos_probs = probs[:, 1]\n",
    "                else:\n",
    "                    pos_probs = probs[:, 0] if clf.classes_[0] == 1 else 0.0\n",
    "                    \n",
    "                df.loc[train_end:test_end, 'Meta_Prob'] = pos_probs\n",
    "        \n",
    "        # --- 4. Signal Construction ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # Confidence Floor: \n",
    "        # If the ML confirms the hybrid signal (Prob > 0.45), we execute.\n",
    "        # This allows RSI Dips to pass IF the ML thinks they are profitable.\n",
    "        active_trade = (df['Primary_Signal'] == 1) & (df['Meta_Prob'] > 0.45)\n",
    "        df.loc[active_trade, 'Signal'] = 1\n",
    "        \n",
    "        # Sizing (Volatility + Confidence)\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=2.0)\n",
    "        ml_scaler = (df['Meta_Prob'] / 0.5).clip(0.5, 2.0)\n",
    "        \n",
    "        # Macro Override\n",
    "        # If Bear Market, we are defensive, BUT we allow Deep Value (RSI < 30) \n",
    "        # to have slightly more room if the ML loves it.\n",
    "        macro_scaler = df['Macro_Trend'].map({1: 1.0, 0: 0.5})\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler * ml_scaler * macro_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94a7c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV7_AdaptiveOptim(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V7 (WFO): Walk-Forward Optimized Strategy.\n",
    "    \n",
    "    Instead of static rules, this strategy runs a 'Tournament' every quarter.\n",
    "    It tests 4 distinct parameter sets (Profiles) on the past 252 days:\n",
    "    \n",
    "    1. Trend_Aggro: Kalman Slope > 0 (No Macro Filter)\n",
    "    2. Trend_Defense: Kalman Slope > 0 AND Macro_Bull (Like V3)\n",
    "    3. Reversion_Deep: RSI < 30 (Buying Crashes)\n",
    "    4. Reversion_Active: RSI < 45 (Buying Dips)\n",
    "    \n",
    "    It selects the Profile with the highest Sharpe Ratio in the lookback window\n",
    "    and uses it for the next execution window.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.01, 0.1 # Q=0.01 makes it slightly more responsive than V6\n",
    "        \n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            \n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "            \n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Global Feature Engineering ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: df['Macro_Trend'] = 1\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. Pre-Calculate Strategy Candidates (Vectorized) ---\n",
    "        # We calculate the raw signals for all profiles upfront\n",
    "        \n",
    "        # Profile 1: Aggressive Trend (Chase the move)\n",
    "        sig_trend_aggro = (df['Kalman_Slope'] > 0).astype(int)\n",
    "        \n",
    "        # Profile 2: Defensive Trend (V3 Style - Only if Macro agrees)\n",
    "        sig_trend_def = ((df['Kalman_Slope'] > 0) & (df['Macro_Trend'] == 1)).astype(int)\n",
    "        \n",
    "        # Profile 3: Deep Reversion (Catch Falling Knife)\n",
    "        sig_rev_deep = (df['RSI'] < 30).astype(int)\n",
    "        \n",
    "        # Profile 4: Active Reversion (Buy Shallow Dips)\n",
    "        sig_rev_active = (df['RSI'] < 45).astype(int)\n",
    "        \n",
    "        # Store in a dict for easy access\n",
    "        candidates = {\n",
    "            'Trend_Aggro': sig_trend_aggro,\n",
    "            'Trend_Defense': sig_trend_def,\n",
    "            'Rev_Deep': sig_rev_deep,\n",
    "            'Rev_Active': sig_rev_active\n",
    "        }\n",
    "        \n",
    "        # --- 3. Walk-Forward Optimization Loop ---\n",
    "        df['Signal'] = 0.0\n",
    "        df['Selected_Profile'] = 'None' # For debugging/analysis\n",
    "        \n",
    "        lookback = 252       # 1 Year Lookback for Optimization\n",
    "        rebalance_freq = 63  # Quarterly Re-optimization\n",
    "        \n",
    "        indices = df.index\n",
    "        daily_returns = df['Returns']\n",
    "        \n",
    "        if len(df) > lookback:\n",
    "            for t in range(lookback, len(df), rebalance_freq):\n",
    "                train_start = indices[t - lookback]\n",
    "                train_end = indices[t]\n",
    "                test_end_idx = min(t + rebalance_freq, len(df))\n",
    "                test_end = indices[test_end_idx - 1]\n",
    "                \n",
    "                # The Tournament: Check Sharpe of each candidate in lookback period\n",
    "                best_score = -999\n",
    "                best_profile = 'Trend_Defense' # Default safety\n",
    "                \n",
    "                lb_returns = daily_returns.loc[train_start:train_end]\n",
    "                \n",
    "                for name, sig_series in candidates.items():\n",
    "                    # Simulate Strategy Return in Lookback\n",
    "                    # Lag signal by 1 to avoid lookahead in backtest\n",
    "                    sigs = sig_series.loc[train_start:train_end].shift(1).fillna(0)\n",
    "                    strat_ret = lb_returns * sigs\n",
    "                    \n",
    "                    # Calculate Metric (Sharpe)\n",
    "                    mean_ret = strat_ret.mean()\n",
    "                    std_ret = strat_ret.std()\n",
    "                    \n",
    "                    if std_ret > 1e-6:\n",
    "                        score = mean_ret / std_ret # Simple Sharpe\n",
    "                    else:\n",
    "                        score = -999 # Flat line is bad\n",
    "                        \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_profile = name\n",
    "                \n",
    "                # Apply Best Profile to Next Window (Test Set)\n",
    "                # We use the signal series for the *future* window based on the *past* winner\n",
    "                winner_signals = candidates[best_profile].loc[train_end:test_end]\n",
    "                df.loc[train_end:test_end, 'Signal'] = winner_signals\n",
    "                df.loc[train_end:test_end, 'Selected_Profile'] = best_profile\n",
    "\n",
    "        # --- 4. Volatility Targeting (Risk Management) ---\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8f7e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV8_GrandUnification(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V8: The Regime-Adaptive Ensemble.\n",
    "    \n",
    "    It combines the strengths of previous iterations:\n",
    "    1. UNSUPERVISED STATE DETECTION (from V1): \n",
    "       Uses Gaussian Mixture Models (GMM) to classify the market into 3 regimes:\n",
    "       - Low Vol / High Return -> \"Stable Bull\"\n",
    "       - High Vol / Negative Return -> \"Crisis/Bear\"\n",
    "       - Medium Vol / Flat Return -> \"Chop\"\n",
    "       \n",
    "    2. CONDITIONAL LOGIC (from V6):\n",
    "       - If State == Stable Bull: Deploy AGGRESSIVE TREND (Kalman Slope).\n",
    "       - If State == Chop: Deploy ACTIVE REVERSION (RSI < 45).\n",
    "       - If State == Crisis: Go CASH (or Deep Value RSI < 25 only).\n",
    "       \n",
    "    3. GLOBAL SAFETY (from V3):\n",
    "       - Overrides everything if SPY is below 200 SMA (Systemic Risk).\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.001, 0.1\n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Data & Features ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: df['Macro_Trend'] = 1\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. GMM Regime Detection (The \"Brain\") ---\n",
    "        # We cluster the market into 3 states based on Return & Risk\n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Train GMM\n",
    "        gmm = GaussianMixture(n_components=3, random_state=42, n_init=10)\n",
    "        df['Cluster'] = gmm.fit_predict(X_scaled)\n",
    "        \n",
    "        # --- 3. Dynamic Profile Mapping ---\n",
    "        # We must figure out which cluster is which (Bull vs Bear vs Chop)\n",
    "        # We assume:\n",
    "        # - Highest Return = Bull\n",
    "        # - Lowest Return = Bear\n",
    "        # - Middle = Chop\n",
    "        \n",
    "        stats = df.groupby('Cluster')['Returns_Smoothed'].mean().sort_values()\n",
    "        bear_cluster = stats.index[0]\n",
    "        chop_cluster = stats.index[1]\n",
    "        bull_cluster = stats.index[2]\n",
    "        \n",
    "        # Map clusters to readable names\n",
    "        conditions = [\n",
    "            (df['Cluster'] == bull_cluster),\n",
    "            (df['Cluster'] == bear_cluster),\n",
    "            (df['Cluster'] == chop_cluster)\n",
    "        ]\n",
    "        choices = ['BULL', 'BEAR', 'CHOP']\n",
    "        df['Regime_Type'] = np.select(conditions, choices, default='CHOP')\n",
    "        \n",
    "        # --- 4. Regime-Conditional Signal Logic ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # A. BULL REGIME: Trend Following\n",
    "        # Use Kalman Slope. If Slope > 0, we ride.\n",
    "        # We ignore RSI overbought because strong trends stay overbought.\n",
    "        bull_signal = (df['Regime_Type'] == 'BULL') & (df['Kalman_Slope'] > 0)\n",
    "        df.loc[bull_signal, 'Signal'] = 1\n",
    "        \n",
    "        # B. CHOP REGIME: Mean Reversion\n",
    "        # Market is going nowhere. Buy dips.\n",
    "        # RSI < 45 is a good entry in chop.\n",
    "        chop_signal = (df['Regime_Type'] == 'CHOP') & (df['RSI'] < 45)\n",
    "        df.loc[chop_signal, 'Signal'] = 1\n",
    "        \n",
    "        # C. BEAR REGIME: Cash / Deep Value\n",
    "        # Mostly Cash. Only buy EXTREME panic (RSI < 25).\n",
    "        # This saved V1 on BABA.\n",
    "        bear_signal = (df['Regime_Type'] == 'BEAR') & (df['RSI'] < 25)\n",
    "        df.loc[bear_signal, 'Signal'] = 1\n",
    "        \n",
    "        # --- 5. Global Safety Filters ---\n",
    "        \n",
    "        # Filter 1: Macro Override (V3)\n",
    "        # If the broad market is crashing (SPY < 200MA), reduce all long exposure by 50%\n",
    "        # or cut entirely if it's a Bear stock in a Bear market.\n",
    "        if 'Macro_Trend' in df.columns:\n",
    "            # If Macro is bad, we kill the 'Chop' and 'Bull' signals for beta stocks\n",
    "            # but we might keep 'Deep Value' signals.\n",
    "            # For simplicity: Scale down everything.\n",
    "            df.loc[df['Macro_Trend'] == 0, 'Signal'] *= 0.0 \n",
    "            # STRICT RULE: No longs in Bear Market. This mimics V3's success.\n",
    "        \n",
    "        # Filter 2: Volatility Targeting\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e2ad3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV9_RegimeUnshackled(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V9: The 'Unshackled' Regime Model.\n",
    "    \n",
    "    Improvements over V8:\n",
    "    1. TRUST THE BULL (Fixes JPM):\n",
    "       - If GMM says 'Stable Bull', we go Long immediately.\n",
    "       - Kalman Slope is demoted from a 'Gatekeeper' to a 'Sizing Booster'.\n",
    "       \n",
    "    2. THE ALPHA CLAUSE (Fixes NVDA):\n",
    "       - If Stock is Bullish but SPY is Bearish (Macro Divergence), we do NOT exit.\n",
    "       - We trade at 50% size. This captures relative strength leaders early.\n",
    "       \n",
    "    3. SURVIVAL MODE (Keeps BABA safe):\n",
    "       - If GMM says 'Bear', we hard-exit to Cash (unless Deep Value RSI < 25).\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.001, 0.1\n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Features ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: df['Macro_Trend'] = 1\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. GMM Regime Detection ---\n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # We use warm_start=True logic simulation by consistent random_state\n",
    "        gmm = GaussianMixture(n_components=3, random_state=42, n_init=10)\n",
    "        df['Cluster'] = gmm.fit_predict(X_scaled)\n",
    "        \n",
    "        # Dynamic Mapping\n",
    "        stats = df.groupby('Cluster')['Returns_Smoothed'].mean().sort_values()\n",
    "        bear_cluster = stats.index[0]\n",
    "        chop_cluster = stats.index[1]\n",
    "        bull_cluster = stats.index[2]\n",
    "        \n",
    "        conditions = [\n",
    "            (df['Cluster'] == bull_cluster),\n",
    "            (df['Cluster'] == bear_cluster),\n",
    "            (df['Cluster'] == chop_cluster)\n",
    "        ]\n",
    "        choices = ['BULL', 'BEAR', 'CHOP']\n",
    "        df['Regime_Type'] = np.select(conditions, choices, default='CHOP')\n",
    "        \n",
    "        # --- 3. Unshackled Signal Logic ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # A. BULL REGIME: \"Trust The Trend\"\n",
    "        # If GMM says Bull, we are Long. Period.\n",
    "        # This captures JPM's \"slow grind\" that Kalman missed.\n",
    "        bull_signal = (df['Regime_Type'] == 'BULL')\n",
    "        df.loc[bull_signal, 'Signal'] = 1.0\n",
    "        \n",
    "        # Boost: If Kalman agrees (Strong Trend), we go 1.3x leverage\n",
    "        strong_trend = bull_signal & (df['Kalman_Slope'] > 0)\n",
    "        df.loc[strong_trend, 'Signal'] = 1.3\n",
    "        \n",
    "        # B. CHOP REGIME: \"Active Trading\"\n",
    "        # Buy Dips.\n",
    "        chop_buy = (df['Regime_Type'] == 'CHOP') & (df['RSI'] < 45)\n",
    "        df.loc[chop_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # C. BEAR REGIME: \"Survival\"\n",
    "        # Cash is King. Only buy extreme panic.\n",
    "        panic_buy = (df['Regime_Type'] == 'BEAR') & (df['RSI'] < 25)\n",
    "        df.loc[panic_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # --- 4. The Alpha Clause (Macro Handling) ---\n",
    "        \n",
    "        # Standard Volatility Sizing (Risk Parity)\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        # Macro Logic:\n",
    "        # If SPY is Bearish (0), we don't kill the trade. We just HALVE it.\n",
    "        # This allows NVDA to run while still being defensive.\n",
    "        macro_scaler = df['Macro_Trend'].map({1: 1.0, 0: 0.5})\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler * macro_scaler\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0729",
   "metadata": {},
   "source": [
    "## QINGYANG LUCAS FANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f311e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U0_MeanReversion(BaseStrategy):\n",
    "    \"\"\"\n",
    "    U0: Granular Mean Reversion (The \"Wick Filter\").\n",
    "    Fixed for Yahoo Finance 730-day hourly limit.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, sensitivity=20, period=14, k=2.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.sensitivity = sensitivity\n",
    "        self.period = period\n",
    "        self.k = k\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # NOTE: Yahoo Finance 1h data is STRICTLY limited to the last 730 days.\n",
    "        # We must clamp the start_date to fit this constraint.\n",
    "        \n",
    "        # Calculate the requested start date with warmup\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        \n",
    "        # Calculate the 730-day hard limit (with a small buffer)\n",
    "        limit_dt = datetime.now() - timedelta(days=725)\n",
    "        \n",
    "        # Enforce limit\n",
    "        if start_dt < limit_dt:\n",
    "            print(f\"Warning: {self.ticker} start date {start_dt.date()} exceeds 730-day hourly limit. Truncating to {limit_dt.date()}.\")\n",
    "            start_dt = limit_dt\n",
    "        \n",
    "        start_str = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Get Hourly Data (Feature Construction)\n",
    "            df_h = yf.download(self.ticker, interval=\"1h\", start=start_str, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            \n",
    "            # Handle empty download immediately\n",
    "            if df_h.empty:\n",
    "                print(f\"Error: No hourly data found for {self.ticker}\")\n",
    "                self.data = pd.DataFrame()\n",
    "                return\n",
    "\n",
    "            if isinstance(df_h.columns, pd.MultiIndex): df_h.columns = df_h.columns.get_level_values(0)\n",
    "            df_h = df_h[['Close']].dropna()\n",
    "            df_h.index = df_h.index.tz_localize(None)\n",
    "\n",
    "            # 2. Resample to Daily\n",
    "            daily_max = df_h['Close'].resample('1D').max()\n",
    "            daily_min = df_h['Close'].resample('1D').min()\n",
    "            daily_diff = daily_max - daily_min\n",
    "            \n",
    "            # 3. Get Standard Daily Data (Backtest alignment)\n",
    "            df_d = yf.download(self.ticker, interval=\"1d\", start=start_str, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            if df_d.empty:\n",
    "                self.data = pd.DataFrame()\n",
    "                return\n",
    "                \n",
    "            if isinstance(df_d.columns, pd.MultiIndex): df_d.columns = df_d.columns.get_level_values(0)\n",
    "            df_d['Returns'] = df_d['Close'].pct_change()\n",
    "            \n",
    "            # 4. Merge\n",
    "            df_d['Daily_Max_H'] = daily_max\n",
    "            df_d['Daily_Min_H'] = daily_min\n",
    "            df_d['Daily_Diff_H'] = daily_diff\n",
    "            \n",
    "            df_d.dropna(inplace=True)\n",
    "            self.data = df_d\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for U0 {self.ticker}: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # 1. Features\n",
    "        df['ATR'] = df['Daily_Diff_H'].rolling(self.period).mean()\n",
    "        df['Roll_Max'] = df['Daily_Max_H'].rolling(self.sensitivity).max()\n",
    "        df['Roll_Min'] = df['Daily_Min_H'].rolling(self.sensitivity).min()\n",
    "        \n",
    "        # 2. SHIFT (No Look-Ahead)\n",
    "        feat_cols = ['ATR', 'Roll_Max', 'Roll_Min', 'Daily_Max_H', 'Daily_Min_H']\n",
    "        df[feat_cols] = df[feat_cols].shift(1)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 3. Bands\n",
    "        df['Upper_Band'] = df['Roll_Max'] + self.k * df['ATR']\n",
    "        df['Lower_Band'] = df['Roll_Min'] - self.k * df['ATR']\n",
    "        \n",
    "        # 4. Logic\n",
    "        close = df['Close']\n",
    "        long_entry = close < df['Lower_Band']\n",
    "        short_entry = close > df['Upper_Band']\n",
    "        long_exit = close > df['Daily_Max_H']\n",
    "        short_exit = close < df['Daily_Min_H']\n",
    "        \n",
    "        # 5. State Machine\n",
    "        pos = 0\n",
    "        signals = []\n",
    "        le_arr, se_arr = long_entry.values, short_entry.values\n",
    "        lx_arr, sx_arr = long_exit.values, short_exit.values\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if pos == 0:\n",
    "                if le_arr[i]: pos = 1\n",
    "                elif se_arr[i]: pos = -1\n",
    "            elif pos == 1:\n",
    "                if lx_arr[i]: pos = 0\n",
    "            elif pos == -1:\n",
    "                if sx_arr[i]: pos = 0\n",
    "            signals.append(pos)\n",
    "            \n",
    "        df['Signal'] = signals\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9598e",
   "metadata": {},
   "source": [
    "## 5. ROBUST BENCHMARK INFRASTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "285c817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustBenchmark:\n",
    "    \"\"\"\n",
    "    Implements Walk-Forward Analysis and Deflated Sharpe Ratio logic.\n",
    "    Benchmarks multiple strategies without look-ahead bias[cite: 275].\n",
    "    \"\"\"\n",
    "    def __init__(self, tickers, start_date, end_date):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.results = []\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"{'STRATEGY':<10} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        strategies = {\n",
    "            \"V1_Base\": StrategyV1_Baseline,\n",
    "            # \"V2_GMM\": StrategyV2_Advanced,\n",
    "            \"V3_Macro\": StrategyV3_Macro,\n",
    "            # \"V4_Meta\": StrategyV4_Meta,\n",
    "            # \"V5_Kalman\": StrategyV5_KalmanState,\n",
    "            # \"V6_Inst\": StrategyV6_MetaLabeling,\n",
    "            \"V7_Optim\": StrategyV7_AdaptiveOptim,\n",
    "            \"V8_Final\": StrategyV8_GrandUnification,\n",
    "            \"V9_Unshack\": StrategyV9_RegimeUnshackled,\n",
    "            \"U0_MeanRev\": U0_MeanReversion\n",
    "        }\n",
    "\n",
    "        for ticker in self.tickers:\n",
    "            # Capture Buy & Hold first\n",
    "            bh = StrategyV1_Baseline(ticker, self.start_date, self.end_date)\n",
    "            bh.fetch_data()\n",
    "            bh.data['Signal'] = 1 # Force Buy\n",
    "            bh.run_backtest()\n",
    "            self._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "            \n",
    "            for name, StratClass in strategies.items():\n",
    "                try:\n",
    "                    strat = StratClass(ticker, self.start_date, self.end_date)\n",
    "                    strat.fetch_data(warmup_years=2)\n",
    "                    strat.generate_signals()\n",
    "                    strat.run_backtest()\n",
    "                    \n",
    "                    self._print_row(name, ticker, strat.metrics)\n",
    "                    \n",
    "                    # Store for portfolio level (optional)\n",
    "                    self.results.append({\n",
    "                        'Ticker': ticker,\n",
    "                        'Strategy': name,\n",
    "                        'Returns': strat.results['Net_Returns']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed {name} {ticker}: {e}\")\n",
    "            print(\"-\" * 75)\n",
    "\n",
    "    def _print_row(self, name, ticker, metrics):\n",
    "        if not metrics: return\n",
    "        ret = metrics['Total Return']\n",
    "        # Annualize return approx\n",
    "        ann_ret = (1 + ret) ** (252 / len(metrics.get('Returns', [1]*252))) - 1 if 'Returns' in metrics else ret\n",
    "        print(f\"{name:<10} | {ticker:<6} | {ret:.1%}   | {metrics['Sharpe Ratio']:.2f}   | {metrics['Max Drawdown']:.1%}   |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c992984",
   "metadata": {},
   "source": [
    "## 6. EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99c17056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers = [\n",
    "#     \"NVDA\", # Tech Momentum\n",
    "#     \"JPM\",  # Financial/Value\n",
    "#     \"TSLA\", # High Volatility/Cult\n",
    "#     \"KO\",   # Defensive/Staples\n",
    "#     \"MSTR\", # Crypto/Hyper-Trend\n",
    "#     \"XLE\",  # Energy/Cyclical\n",
    "#     \"BABA\"  # Distressed/Bear\n",
    "# ]\n",
    "\n",
    "# bench = RobustBenchmark(\n",
    "#     tickers=tickers, \n",
    "#     start_date=\"2022-01-01\", \n",
    "#     end_date=\"2024-12-30\"\n",
    "# )\n",
    "# bench.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f62c7e",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80c0270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble(BaseStrategy):\n",
    "    \"\"\"\n",
    "    The 'All-Weather' Ensemble.\n",
    "    \n",
    "    Combines V3 (Macro Trend) and V9 (Regime Unshackled) into a single\n",
    "    portfolio-level signal.\n",
    "    \n",
    "    Logic:\n",
    "    1. Runs V3 to capture high-beta trends (NVDA, Bitcoin).\n",
    "    2. Runs V9 to capture regime-based alpha and protect downside (JPM, BABA).\n",
    "    3. Blends signals using a 'Correlation-Adjusted' weighting or fixed 50/50.\n",
    "    4. Applies a final Volatility Target to the combined equity curve to ensure\n",
    "       the two strategies don't stack up to dangerous leverage.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, w_v3=0.5, w_v9=0.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.w_v3 = w_v3\n",
    "        self.w_v9 = w_v9\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Fetch once for efficiency (logic could be optimized to share DF, \n",
    "        # but separate fetch ensures cleaner encapsulation)\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # We share the index/data from one of them for the main wrapper\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Align Indices (Inner Join to be safe)\n",
    "        df = self.data.copy()\n",
    "        s3 = self.strat_v3.data['Signal']\n",
    "        s9 = self.strat_v9.data['Signal']\n",
    "        \n",
    "        # Merge signals into main DF\n",
    "        df['Sig_V3'] = s3\n",
    "        df['Sig_V9'] = s9\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. The Allocation Logic\n",
    "        # Default: Fixed Weight (Core-Satellite Approach)\n",
    "        # V3 (Beta) + V9 (Alpha)\n",
    "        \n",
    "        # We blend the RAW signals.\n",
    "        # Note: Signals are already Vol-Targeted to ~15% inside sub-classes.\n",
    "        # Simple addition would double vol if correlation=1.\n",
    "        raw_blend = (df['Sig_V3'] * self.w_v3) + (df['Sig_V9'] * self.w_v9)\n",
    "        \n",
    "        # 3. Ensemble Volatility Control\n",
    "        # If V3 and V9 agree (both Long), we get high exposure.\n",
    "        # If they disagree (V3 Long, V9 Cash), we get half exposure.\n",
    "        # This naturally deleverages during uncertainty.\n",
    "        \n",
    "        df['Signal'] = raw_blend\n",
    "        \n",
    "        # Optional: Re-Target Volatility of the *Ensemble*\n",
    "        # (Prevents leverage creep if strategies are highly correlated)\n",
    "        # For now, we trust the weighted sum to act as a diversification benefit.\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd66f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble_Adaptive(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V10: The Adaptive Ensemble (Dynamic Weighting).\n",
    "    \n",
    "    Instead of fixed weights, this strategy re-allocates capital quarterly \n",
    "    based on the recent Risk-Adjusted Performance (Sharpe) of the sub-strategies.\n",
    "    \n",
    "    Logic:\n",
    "    1. Lookback: 126 Days (6 Months).\n",
    "    2. Rebalance: Every 63 Days (Quarterly).\n",
    "    3. Weighting:\n",
    "       - Calculate Sharpe Ratio for V3 and V9 in the lookback window.\n",
    "       - If Sharpe > 0: Weight is proportional to Sharpe.\n",
    "       - If Sharpe < 0: Weight is set to 0.\n",
    "       - Normalize weights to sum to 1.0.\n",
    "       \n",
    "    This allows the portfolio to automatically 'Risk On' into V3 during strong bulls\n",
    "    and 'Risk Off' into V9 during bears/chop.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # Use one of the dataframes as the base\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Merge Data\n",
    "        df = self.data.copy()\n",
    "        df['Sig_V3'] = self.strat_v3.data['Signal']\n",
    "        df['Sig_V9'] = self.strat_v9.data['Signal']\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. Simulate Sub-Strategy Returns (for metric calculation)\n",
    "        # We need to know how they *would* have performed to weight them.\n",
    "        # Lag signals by 1 to avoid lookahead bias when calculating returns.\n",
    "        df['Ret_V3'] = df['Sig_V3'].shift(1) * df['Returns']\n",
    "        df['Ret_V9'] = df['Sig_V9'].shift(1) * df['Returns']\n",
    "        \n",
    "        # 3. Walk-Forward Weight Optimization\n",
    "        df['W_V3'] = 0.5 # Default start\n",
    "        df['W_V9'] = 0.5\n",
    "        \n",
    "        lookback = 126      # 6 Months Lookback\n",
    "        rebalance_freq = 21 # Monthly Rebalance (Faster adaptation)\n",
    "        \n",
    "        indices = df.index\n",
    "        \n",
    "        if len(df) > lookback:\n",
    "            for t in range(lookback, len(df), rebalance_freq):\n",
    "                train_start = indices[t - lookback]\n",
    "                train_end = indices[t]\n",
    "                test_end_idx = min(t + rebalance_freq, len(df))\n",
    "                test_end = indices[test_end_idx - 1]\n",
    "                \n",
    "                # Calculate Sharpe in Lookback Window\n",
    "                # Add small epsilon to std to avoid division by zero\n",
    "                v3_mean = df.loc[train_start:train_end, 'Ret_V3'].mean()\n",
    "                v3_std = df.loc[train_start:train_end, 'Ret_V3'].std() + 1e-9\n",
    "                sharpe_v3 = (v3_mean / v3_std) * np.sqrt(252)\n",
    "                \n",
    "                v9_mean = df.loc[train_start:train_end, 'Ret_V9'].mean()\n",
    "                v9_std = df.loc[train_start:train_end, 'Ret_V9'].std() + 1e-9\n",
    "                sharpe_v9 = (v9_mean / v9_std) * np.sqrt(252)\n",
    "                \n",
    "                # Weighting Logic\n",
    "                # 1. Filter: If Sharpe is negative, set score to 0\n",
    "                score_v3 = max(0, sharpe_v3)\n",
    "                score_v9 = max(0, sharpe_v9)\n",
    "                \n",
    "                # 2. Normalize\n",
    "                total_score = score_v3 + score_v9\n",
    "                \n",
    "                if total_score > 0:\n",
    "                    w_v3 = score_v3 / total_score\n",
    "                    w_v9 = score_v9 / total_score\n",
    "                else:\n",
    "                    # Both are failing? Default to Defensive (V9) or Cash (0)\n",
    "                    # Let's default to V9 (Safety) as the 'bunker'\n",
    "                    w_v3 = 0.0\n",
    "                    w_v9 = 1.0\n",
    "                \n",
    "                # Apply weights to NEXT window\n",
    "                df.loc[train_end:test_end, 'W_V3'] = w_v3\n",
    "                df.loc[train_end:test_end, 'W_V9'] = w_v9\n",
    "                \n",
    "        # 4. Final Signal Generation\n",
    "        # Blend the signals using the dynamic weights\n",
    "        df['Signal'] = (df['Sig_V3'] * df['W_V3']) + (df['Sig_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e70d03",
   "metadata": {},
   "source": [
    "## HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39523e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U0_MeanReversion(BaseStrategy):\n",
    "    \"\"\"\n",
    "    U0: Hourly-Granularity Mean Reversion.\n",
    "    Uses 'Wick-Free' Daily Highs/Lows derived from Hourly closes.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, sensitivity=20, period=14, k=2.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.sensitivity = sensitivity\n",
    "        self.period = period\n",
    "        self.k = k\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Clamp start date to Yahoo's 730-day hourly limit\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        limit_dt = datetime.now() - timedelta(days=725)\n",
    "        if start_dt < limit_dt: start_dt = limit_dt\n",
    "        \n",
    "        try:\n",
    "            # 1. Hourly Data\n",
    "            df_h = yf.download(self.ticker, interval=\"1h\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=True)\n",
    "            if df_h.empty: return\n",
    "            if isinstance(df_h.columns, pd.MultiIndex): df_h.columns = df_h.columns.get_level_values(0)\n",
    "            df_h = df_h[['Close']].dropna()\n",
    "            df_h.index = df_h.index.tz_localize(None)\n",
    "\n",
    "            # 2. Custom Daily Aggregation\n",
    "            daily_max = df_h['Close'].resample('1D').max()\n",
    "            daily_min = df_h['Close'].resample('1D').min()\n",
    "            daily_diff = daily_max - daily_min\n",
    "            \n",
    "            # 3. Standard Daily Data\n",
    "            df_d = yf.download(self.ticker, interval=\"1d\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=True)\n",
    "            if isinstance(df_d.columns, pd.MultiIndex): df_d.columns = df_d.columns.get_level_values(0)\n",
    "            \n",
    "            df_d['Returns'] = df_d['Close'].pct_change()\n",
    "            df_d['Daily_Max_H'] = daily_max\n",
    "            df_d['Daily_Min_H'] = daily_min\n",
    "            df_d['Daily_Diff_H'] = daily_diff\n",
    "            df_d.dropna(inplace=True)\n",
    "            self.data = df_d\n",
    "        except: self.data = pd.DataFrame()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # Features\n",
    "        df['ATR'] = df['Daily_Diff_H'].rolling(self.period).mean()\n",
    "        df['Roll_Max'] = df['Daily_Max_H'].rolling(self.sensitivity).max()\n",
    "        df['Roll_Min'] = df['Daily_Min_H'].rolling(self.sensitivity).min()\n",
    "        \n",
    "        # Shift (No Lookahead)\n",
    "        cols = ['ATR', 'Roll_Max', 'Roll_Min', 'Daily_Max_H', 'Daily_Min_H']\n",
    "        df[cols] = df[cols].shift(1)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Bands\n",
    "        df['Upper'] = df['Roll_Max'] + self.k * df['ATR']\n",
    "        df['Lower'] = df['Roll_Min'] - self.k * df['ATR']\n",
    "        \n",
    "        # State Machine Vectorization\n",
    "        pos = 0\n",
    "        signals = []\n",
    "        close = df['Close'].values\n",
    "        lower = df['Lower'].values\n",
    "        upper = df['Upper'].values\n",
    "        dmax = df['Daily_Max_H'].values\n",
    "        dmin = df['Daily_Min_H'].values\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if pos == 0:\n",
    "                if close[i] < lower[i]: pos = 1\n",
    "                elif close[i] > upper[i]: pos = -1\n",
    "            elif pos == 1:\n",
    "                if close[i] > dmax[i]: pos = 0\n",
    "            elif pos == -1:\n",
    "                if close[i] < dmin[i]: pos = 0\n",
    "            signals.append(pos)\n",
    "            \n",
    "        df['Signal'] = signals\n",
    "        # Normalize signal to same scale as V3/V9 (approx 0 to 1 leverage)\n",
    "        # U0 is aggressive, so we scale it down slightly\n",
    "        df['Signal'] = df['Signal'] * 0.7 \n",
    "        self.data = df\n",
    "\n",
    "# ==========================================\n",
    "# 3. HRP MATH ENGINE (The Allocator)\n",
    "# ==========================================\n",
    "class HRP_Allocator:\n",
    "    @staticmethod\n",
    "    def getIVP(cov):\n",
    "        ivp = 1. / np.diag(cov)\n",
    "        ivp /= ivp.sum()\n",
    "        return ivp\n",
    "\n",
    "    @staticmethod\n",
    "    def getClusterVar(cov, cItems):\n",
    "        cov_ = cov.loc[cItems, cItems] \n",
    "        w_ = HRP_Allocator.getIVP(cov_).reshape(-1, 1)\n",
    "        cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
    "        return cVar\n",
    "\n",
    "    @staticmethod\n",
    "    def getQuasiDiag(link):\n",
    "        link = link.astype(int)\n",
    "        sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "        numItems = link[-1, 3] \n",
    "        while sortIx.max() >= numItems:\n",
    "            sortIx.index = range(0, sortIx.shape[0] * 2, 2) \n",
    "            df0 = sortIx[sortIx >= numItems] \n",
    "            i = df0.index\n",
    "            j = df0.values - numItems\n",
    "            sortIx[i] = link[j, 0] \n",
    "            df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "            sortIx = sortIx.append(df0) \n",
    "            sortIx = sortIx.sort_index() \n",
    "            sortIx.index = range(sortIx.shape[0]) \n",
    "        return sortIx.tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def getRecBipart(cov, sortIx):\n",
    "        w = pd.Series(1, index=sortIx)\n",
    "        cItems = [sortIx] \n",
    "        while len(cItems) > 0:\n",
    "            cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
    "            for i in range(0, len(cItems), 2):\n",
    "                cItems0 = cItems[i] \n",
    "                cItems1 = cItems[i + 1] \n",
    "                cVar0 = HRP_Allocator.getClusterVar(cov, cItems0)\n",
    "                cVar1 = HRP_Allocator.getClusterVar(cov, cItems1)\n",
    "                alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "                w[cItems0] *= alpha \n",
    "                w[cItems1] *= 1 - alpha \n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize(returns_df):\n",
    "        # Handle constant/zero returns to avoid NaN correlations\n",
    "        if returns_df.std().min() < 1e-6:\n",
    "            return pd.Series(1/len(returns_df.columns), index=returns_df.columns)\n",
    "            \n",
    "        corr = returns_df.corr().fillna(0)\n",
    "        cov = returns_df.cov().fillna(0)\n",
    "        dist = ((1 - corr) / 2.) ** .5\n",
    "        link = sch.linkage(dist, 'single')\n",
    "        sortIx = HRP_Allocator.getQuasiDiag(link)\n",
    "        sortIx = corr.index[sortIx].tolist()\n",
    "        return HRP_Allocator.getRecBipart(cov, sortIx)\n",
    "\n",
    "class Strategy_Ensemble_HRP(BaseStrategy):\n",
    "    \"\"\"\n",
    "    Phase 1 Upgrade: The HRP Ensemble.\n",
    "    Allocates capital dynamically between V3 (Trend), V9 (Regime), and U0 (Mean Rev)\n",
    "    based on their CORRELATION structure, not just returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        self.u0 = U0_MeanReversion(ticker, start_date, end_date)\n",
    "    \n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        self.u0.fetch_data(warmup_years) # U0 has strictest data reqs\n",
    "        if self.u0.data is None or self.u0.data.empty: return\n",
    "        \n",
    "        # Sync V3/V9 to U0's available timeframe\n",
    "        valid_start = self.u0.data.index[0].strftime(\"%Y-%m-%d\")\n",
    "        self.v3.start_date = valid_start\n",
    "        self.v9.start_date = valid_start\n",
    "        \n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        self.data = self.u0.data.copy() # Use U0 as master index\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.u0.data is None: return\n",
    "        \n",
    "        # Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        self.u0.generate_signals()\n",
    "        \n",
    "        # Merge Signals\n",
    "        df = self.data.copy()\n",
    "        # Join by index to handle missing rows\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='inner')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='inner')\n",
    "        df = df.join(self.u0.data[['Signal']].rename(columns={'Signal':'S_U0'}), how='inner')\n",
    "        \n",
    "        # Simulate Returns for HRP\n",
    "        df['R_V3'] = df['S_V3'].shift(1) * df['Returns']\n",
    "        df['R_V9'] = df['S_V9'].shift(1) * df['Returns']\n",
    "        df['R_U0'] = df['S_U0'].shift(1) * df['Returns']\n",
    "        \n",
    "        # HRP Walk-Forward Loop\n",
    "        lookback = 63  # Short lookback for correlation (3 months)\n",
    "        rebal = 21     # Monthly rebalance\n",
    "        \n",
    "        df['W_V3'], df['W_V9'], df['W_U0'] = 0.33, 0.33, 0.33\n",
    "        \n",
    "        indices = df.index\n",
    "        for t in range(lookback, len(df), rebal):\n",
    "            start = indices[t-lookback]\n",
    "            end = indices[t]\n",
    "            test_end = indices[min(t+rebal, len(df)-1)]\n",
    "            \n",
    "            # Extract Returns History for Optimization\n",
    "            ret_hist = df.loc[start:end, ['R_V3', 'R_V9', 'R_U0']]\n",
    "            \n",
    "            # Run HRP\n",
    "            try:\n",
    "                weights = HRP_Allocator.optimize(ret_hist)\n",
    "                df.loc[end:test_end, 'W_V3'] = weights['R_V3']\n",
    "                df.loc[end:test_end, 'W_V9'] = weights['R_V9']\n",
    "                df.loc[end:test_end, 'W_U0'] = weights['R_U0']\n",
    "            except: pass\n",
    "            \n",
    "        # Final Ensemble Signal\n",
    "        df['Signal'] = (df['S_V3']*df['W_V3']) + (df['S_V9']*df['W_V9']) + (df['S_U0']*df['W_U0'])\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8caaba",
   "metadata": {},
   "source": [
    "## HRP Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fc549e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV11_HRP_Momentum(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V11: HRP + Momentum (The 'Smart' Allocator).\n",
    "    \n",
    "    Fixes the 'Conservative Bias' of standard HRP.\n",
    "    \n",
    "    Logic:\n",
    "    1. Risk Engine (HRP): Calculates base weights to minimize portfolio variance \n",
    "       and handle correlation clustering (separating Trend vs. Mean Rev).\n",
    "    2. Return Engine (Momentum): Calculates the rolling Sharpe Ratio of each strategy.\n",
    "    3. Fusion: \n",
    "       Final_Weight = HRP_Weight * (1 + Sharpe_Score)\n",
    "       \n",
    "    This ensures we diversify risk BUT aggressively overweight the strategies \n",
    "    that are actually making money right now.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        self.u0 = U0_MeanReversion(ticker, start_date, end_date) # Our uncorrelated hedge\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Fetch U0 first (strict data limits)\n",
    "        self.u0.fetch_data(warmup_years)\n",
    "        if self.u0.data is None or self.u0.data.empty: return\n",
    "        \n",
    "        valid_start = self.u0.data.index[0].strftime(\"%Y-%m-%d\")\n",
    "        self.v3.start_date = valid_start\n",
    "        self.v9.start_date = valid_start\n",
    "        \n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        self.data = self.u0.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.u0.data is None: return\n",
    "        \n",
    "        # 1. Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        self.u0.generate_signals()\n",
    "        \n",
    "        # 2. Merge Signals & Sync Index\n",
    "        df = self.data.copy()\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='inner')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='inner')\n",
    "        df = df.join(self.u0.data[['Signal']].rename(columns={'Signal':'S_U0'}), how='inner')\n",
    "        \n",
    "        # 3. Simulate Returns (Needed for Correlation & Sharpe)\n",
    "        df['R_V3'] = df['S_V3'].shift(1) * df['Returns']\n",
    "        df['R_V9'] = df['S_V9'].shift(1) * df['Returns']\n",
    "        df['R_U0'] = df['S_U0'].shift(1) * df['Returns']\n",
    "        \n",
    "        # 4. HRP-Momentum Optimization Loop\n",
    "        lookback = 63  # 3 Months for correlation structure\n",
    "        rebal = 21     # Monthly rebalance\n",
    "        \n",
    "        # Initialize with equal weights (fallback)\n",
    "        df['W_V3'], df['W_V9'], df['W_U0'] = 0.33, 0.33, 0.33\n",
    "        \n",
    "        indices = df.index\n",
    "        # Start loop only after lookback is satisfied\n",
    "        if len(df) > lookback:\n",
    "            for t in range(lookback, len(df), rebal):\n",
    "                start = indices[t-lookback]\n",
    "                end = indices[t]\n",
    "                test_end = indices[min(t+rebal, len(df)-1)]\n",
    "                \n",
    "                # A. Get Returns History\n",
    "                hist = df.loc[start:end, ['R_V3', 'R_V9', 'R_U0']]\n",
    "                \n",
    "                # B. Calculate HRP Base Weights (Risk Parity)\n",
    "                # This minimizes the 'Cluster Variance'\n",
    "                try:\n",
    "                    hrp_w = HRP_Allocator.optimize(hist)\n",
    "                except:\n",
    "                    hrp_w = pd.Series([0.33, 0.33, 0.33], index=['R_V3','R_V9','R_U0'])\n",
    "                \n",
    "                # C. Calculate Momentum Boost (Sharpe Ratio)\n",
    "                means = hist.mean()\n",
    "                stds = hist.std() + 1e-9\n",
    "                sharpes = (means / stds) * np.sqrt(252)\n",
    "                \n",
    "                # Clip negative Sharpes to 0 (don't fund losers)\n",
    "                mom_score = sharpes.clip(lower=0) \n",
    "                \n",
    "                # D. Fusion Logic\n",
    "                # Combine Risk Weight * (1 + Momentum Score)\n",
    "                # This boosts high-sharpe assets while respecting correlation clusters\n",
    "                raw_w = hrp_w * (1 + mom_score)\n",
    "                \n",
    "                # Normalize to sum to 1.0\n",
    "                final_w = raw_w / raw_w.sum()\n",
    "                \n",
    "                # Fill NaNs (if sum was 0) with Safety (V9)\n",
    "                if final_w.isnull().any():\n",
    "                    final_w = pd.Series({'R_V3':0.0, 'R_V9':1.0, 'R_U0':0.0})\n",
    "                    \n",
    "                # Apply weights\n",
    "                df.loc[end:test_end, 'W_V3'] = final_w['R_V3']\n",
    "                df.loc[end:test_end, 'W_V9'] = final_w['R_V9']\n",
    "                df.loc[end:test_end, 'W_U0'] = final_w['R_U0']\n",
    "\n",
    "        # 5. Final Signal\n",
    "        df['Signal'] = (df['S_V3']*df['W_V3']) + (df['S_V9']*df['W_V9']) + (df['S_U0']*df['W_U0'])\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5892e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV12_Macro_Switch(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V12: The Macro-Guided Ensemble.\n",
    "    \n",
    "    Replaces lookback windows with Real-Time Economic Data.\n",
    "    \n",
    "    DATA SOURCES (Yahoo Finance):\n",
    "    1. ^VIX: CBOE Volatility Index.\n",
    "    2. ^TNX: 10-Year Treasury Yield.\n",
    "    \n",
    "    LOGIC:\n",
    "    1. Calculate 'Macro Stress Score' (0.0 to 1.0).\n",
    "       - VIX Component: Normalized against recent history. High VIX = High Stress.\n",
    "       - Yield Component: Rate of Change (ROC) of TNX. Spiking rates = High Stress.\n",
    "    \n",
    "    2. Dynamic Weighting:\n",
    "       - Weight_V3 (Trend) = 1.0 - Stress_Score\n",
    "       - Weight_V9 (Safety) = Stress_Score\n",
    "       \n",
    "    HYPOTHESIS:\n",
    "    VIX and Rates often spike BEFORE the price crash is fully realized. \n",
    "    This allows the model to switch to safety faster than a Moving Average.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        # We store macro data separately\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # 1. Fetch Ticker Data\n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        \n",
    "        if self.v3.data is None or self.v3.data.empty: return\n",
    "        self.data = self.v3.data.copy()\n",
    "        \n",
    "        # 2. Fetch Macro Data (VIX and TNX)\n",
    "        start_dt = (self.data.index[0] - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "        end_dt = self.end_date\n",
    "        \n",
    "        try:\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            \n",
    "            # Cleaning\n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            macro_df = pd.DataFrame(index=self.data.index)\n",
    "            # Align macro data to the ticker's trading days (ffill for holidays)\n",
    "            macro_df['VIX'] = vix['Close'].reindex(self.data.index, method='ffill')\n",
    "            macro_df['TNX'] = tnx['Close'].reindex(self.data.index, method='ffill')\n",
    "            \n",
    "            self.macro_data = macro_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Macro Data Fetch Error: {e}\")\n",
    "            # Fallback: Zero stress\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        \n",
    "        # 1. Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        \n",
    "        # 2. Sync Data\n",
    "        df = self.data.copy()\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='left')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='left')\n",
    "        \n",
    "        # 3. Calculate Macro Stress Score\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # A. VIX Stress (Fear)\n",
    "        # Normalize VIX: If VIX > 30, Stress = 1.0. If VIX < 15, Stress = 0.0.\n",
    "        # Uses a rolling Z-score or simple clamp? Simple clamp is more robust to regime shifts.\n",
    "        macro['VIX_Stress'] = ((macro['VIX'] - 15) / (30 - 15)).clip(0, 1)\n",
    "        \n",
    "        # B. Yield Stress (Rate Shock)\n",
    "        # We care about SPEED of rate rise, not just level.\n",
    "        # Calculate 20-day Rate of Change of TNX\n",
    "        macro['TNX_ROC'] = macro['TNX'].pct_change(20)\n",
    "        # If Yields rise > 10% in a month, that's a shock.\n",
    "        macro['TNX_Stress'] = (macro['TNX_ROC'] / 0.10).clip(0, 1)\n",
    "        \n",
    "        # Combined Stress (Max of either Fear or Rate Shock)\n",
    "        # We use Max because either one can crash the market independently.\n",
    "        macro['Total_Stress'] = macro[['VIX_Stress', 'TNX_Stress']].max(axis=1)\n",
    "        \n",
    "        # 4. Allocate Weights\n",
    "        # Smooth the stress signal to avoid daily jitter (3-day avg)\n",
    "        stress_signal = macro['Total_Stress'].rolling(3).mean().fillna(0)\n",
    "        \n",
    "        df['W_V9'] = stress_signal        # High Stress -> More Safety\n",
    "        df['W_V3'] = 1.0 - stress_signal  # Low Stress -> More Trend\n",
    "        \n",
    "        # 5. Final Signal\n",
    "        df['Signal'] = (df['S_V3'] * df['W_V3']) + (df['S_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "815abc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV13_Correlation_Switch(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V13: The Correlation-Aware 'Router'.\n",
    "    \n",
    "    Dynamically switches between V12 (Macro) and V9 (Internal Regime) based on \n",
    "    the asset's sensitivity to Market Stress (VIX).\n",
    "    \n",
    "    LOGIC:\n",
    "    1. Calculate Rolling Correlation (6-Month) between Asset Returns and VIX Changes.\n",
    "    2. The 'Fragility' Test:\n",
    "       - If Corr(Asset, VIX) < -0.2: Asset crashes when Fear spikes.\n",
    "         -> ROUTE TO V12 (Macro-Guided).\n",
    "       - If Corr(Asset, VIX) >= -0.2: Asset ignores/benefits from Fear.\n",
    "         -> ROUTE TO V9 (Regime Unshackled).\n",
    "         \n",
    "    This ensures we use the Macro Filter for NVDA (where it works) \n",
    "    but turn it off for XLE/BABA (where it fails).\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v12 = StrategyV12_Macro_Switch(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # 1. Fetch Sub-Strategies (V12 fetches Macro Data internally)\n",
    "        self.v12.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        \n",
    "        if self.v12.data is None or self.v12.data.empty: return\n",
    "        self.data = self.v12.data.copy()\n",
    "        \n",
    "        # 2. Extract Macro Data (VIX) from V12 instance\n",
    "        if self.v12.macro_data is not None:\n",
    "            # Reindex to match main data exactly\n",
    "            self.macro_data = self.v12.macro_data.reindex(self.data.index).fillna(method='ffill')\n",
    "        else:\n",
    "            # Fallback\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        \n",
    "        # 1. Run Sub-Strategies\n",
    "        self.v12.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        \n",
    "        # 2. Merge Signals\n",
    "        df = self.data.copy()\n",
    "        df = df.join(self.v12.data[['Signal']].rename(columns={'Signal':'S_V12'}), how='left')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='left')\n",
    "        \n",
    "        # 3. Calculate Correlation Logic\n",
    "        # We check correlation between Asset Daily Returns and VIX Daily Changes\n",
    "        df['Ret'] = df['Returns']\n",
    "        df['VIX_Chg'] = self.macro_data['VIX'].pct_change()\n",
    "        \n",
    "        # Rolling Correlation (126 Days = ~6 Months)\n",
    "        # We want to know: \"In the last 6 months, did this stock act like a Beta stock?\"\n",
    "        df['VIX_Corr'] = df['Ret'].rolling(126).corr(df['VIX_Chg'])\n",
    "        \n",
    "        # 4. The Router Switch\n",
    "        # Threshold: -0.2. \n",
    "        # If corr is more negative than -0.2 (e.g., -0.5), it means strong inverse relationship.\n",
    "        # High Fear = Low Price. -> Use V12.\n",
    "        # If corr is > -0.2 (e.g., 0.1), relation is weak/positive. -> Use V9.\n",
    "        \n",
    "        # Lag the switch by 1 day to avoid lookahead\n",
    "        switch_signal = df['VIX_Corr'].shift(1).fillna(-1.0) # Default to V12 (Safety)\n",
    "        \n",
    "        df['Weight_V12'] = np.where(switch_signal < -0.2, 1.0, 0.0)\n",
    "        df['Weight_V9']  = 1.0 - df['Weight_V12']\n",
    "        \n",
    "        # 5. Final Signal\n",
    "        df['Signal'] = (df['S_V12'] * df['Weight_V12']) + (df['S_V9'] * df['Weight_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6ae43",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "19a2e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATEGY     | TICKER | ANN RET | SHARPE | MAX DD  | NOTES\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | NVDA   | 355.4%   | 1.19   | -62.7%   |\n",
      "V3_Macro   | NVDA   | 173.4%   | 1.51   | -23.1%   |\n",
      "V9_Unshack | NVDA   | 30.4%   | 0.98   | -9.0%   |\n",
      "Ens_Adapt  | NVDA   | 85.2%   | 1.23   | -22.0%   |\n",
      "V11_HRP_Mom | NVDA   | 22.9%   | 1.34   | -10.4%   |\n",
      "V12_Macro  | NVDA   | 125.0%   | 1.76   | -11.2%   |\n",
      "V13_Switch | NVDA   | 125.0%   | 1.76   | -11.2%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | JPM    | 62.1%   | 0.77   | -37.9%   |\n",
      "V3_Macro   | JPM    | 71.9%   | 0.97   | -28.5%   |\n",
      "V9_Unshack | JPM    | 84.4%   | 0.95   | -25.1%   |\n",
      "Ens_Adapt  | JPM    | 75.6%   | 0.93   | -26.9%   |\n",
      "V11_HRP_Mom | JPM    | 19.8%   | 1.42   | -8.0%   |\n",
      "V12_Macro  | JPM    | 74.6%   | 0.92   | -24.8%   |\n",
      "V13_Switch | JPM    | 74.6%   | 0.92   | -24.8%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | TSLA   | 7.9%   | 0.35   | -73.0%   |\n",
      "V3_Macro   | TSLA   | 31.4%   | 0.54   | -31.4%   |\n",
      "V9_Unshack | TSLA   | 42.7%   | 0.92   | -12.5%   |\n",
      "Ens_Adapt  | TSLA   | 35.1%   | 0.69   | -15.5%   |\n",
      "V11_HRP_Mom | TSLA   | -0.3%   | 0.05   | -15.7%   |\n",
      "V12_Macro  | TSLA   | 44.9%   | 0.81   | -25.2%   |\n",
      "V13_Switch | TSLA   | 44.9%   | 0.81   | -25.2%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | BABA   | -26.9%   | 0.06   | -54.0%   |\n",
      "V3_Macro   | BABA   | 0.6%   | 0.10   | -25.3%   |\n",
      "V9_Unshack | BABA   | 32.3%   | 0.89   | -9.2%   |\n",
      "Ens_Adapt  | BABA   | 27.7%   | 0.78   | -9.9%   |\n",
      "V11_HRP_Mom | BABA   | 11.5%   | 1.12   | -9.3%   |\n",
      "V12_Macro  | BABA   | 3.1%   | 0.14   | -19.6%   |\n",
      "V13_Switch | BABA   | 10.4%   | 0.30   | -19.6%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | XLE    | 65.0%   | 0.77   | -26.0%   |\n",
      "V3_Macro   | XLE    | 17.2%   | 0.38   | -21.8%   |\n",
      "V9_Unshack | XLE    | 9.6%   | 0.25   | -27.3%   |\n",
      "Ens_Adapt  | XLE    | 12.6%   | 0.30   | -23.8%   |\n",
      "V11_HRP_Mom | XLE    | -0.7%   | 0.02   | -13.2%   |\n",
      "V12_Macro  | XLE    | 8.6%   | 0.24   | -24.0%   |\n",
      "V13_Switch | XLE    | 7.1%   | 0.21   | -27.6%   |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"{'STRATEGY':<12} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "    print(\"-\" * 79)\n",
    "    \n",
    "    # Helper wrappers for the dictionary\n",
    "    class Strategy_Ensemble_5050(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.5, 0.5)\n",
    "\n",
    "    class Strategy_Ensemble_Growth(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.7, 0.3)\n",
    "\n",
    "    strategies = {\n",
    "        \"V3_Macro\": StrategyV3_Macro,\n",
    "        \"V9_Unshack\": StrategyV9_RegimeUnshackled,\n",
    "        # \"Ens_Bal\": Strategy_Ensemble_5050,      # Static 50/50\n",
    "        # \"Ens_Grow\": Strategy_Ensemble_Growth,   # Static 70/30\n",
    "        \"Ens_Adapt\": Strategy_Ensemble_Adaptive, # Dynamic V10\n",
    "        # \"HRP_Base\": Strategy_Ensemble_HRP,\n",
    "        \"V11_HRP_Mom\": StrategyV11_HRP_Momentum,\n",
    "        \"V12_Macro\": StrategyV12_Macro_Switch,\n",
    "        \"V13_Switch\": StrategyV13_Correlation_Switch,\n",
    "    }\n",
    "\n",
    "    # Same Stress Test Basket\n",
    "    tickers = [\"NVDA\", \"JPM\", \"TSLA\", \"BABA\", \"XLE\"]\n",
    "\n",
    "    bench = RobustBenchmark(\n",
    "        tickers=tickers, \n",
    "        start_date=\"2022-01-01\", \n",
    "        end_date=\"2024-12-30\"\n",
    "    )\n",
    "    \n",
    "    # Manual run loop to handle the specific classes\n",
    "    for ticker in tickers:\n",
    "        # Buy & Hold\n",
    "        bh = StrategyV1_Baseline(ticker, bench.start_date, bench.end_date)\n",
    "        bh.fetch_data()\n",
    "        bh.data['Signal'] = 1\n",
    "        bh.run_backtest()\n",
    "        bench._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "        \n",
    "        for name, StratClass in strategies.items():\n",
    "            try:\n",
    "                strat = StratClass(ticker, bench.start_date, bench.end_date)\n",
    "                strat.fetch_data(warmup_years=2)\n",
    "                strat.generate_signals()\n",
    "                strat.run_backtest()\n",
    "                bench._print_row(name, ticker, strat.metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Err {name} {ticker}: {e}\")\n",
    "        print(\"-\" * 79)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
