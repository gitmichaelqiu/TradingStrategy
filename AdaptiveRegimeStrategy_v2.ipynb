{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310fc6bf",
   "metadata": {},
   "source": [
    "# AdaptiveRegimeStrategy V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4be3e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db667cc",
   "metadata": {},
   "source": [
    "## Feature Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14adbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLab:\n",
    "    \"\"\"Shared mathematical engine for technical and statistical features.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_weights_frac_diff(d, size, threshold=1e-5):\n",
    "        w = [1.0]\n",
    "        for k in range(1, size):\n",
    "            w_k = -w[-1] / k * (d - k + 1)\n",
    "            w.append(w_k)\n",
    "        w = np.array(w[::-1])\n",
    "        w = w[np.abs(w) > threshold]\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def frac_diff_fixed(series, d, window=50):\n",
    "        # Solves Stationarity Dilemma [cite: 61]\n",
    "        weights = FeatureLab.get_weights_frac_diff(d, window)\n",
    "        res = series.rolling(window=len(weights)).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def yang_zhang_volatility(df, window=30):\n",
    "        # Captures intraday energy/gaps [cite: 82]\n",
    "        log_ho = (df['High'] / df['Open']).apply(np.log)\n",
    "        log_lo = (df['Low'] / df['Open']).apply(np.log)\n",
    "        log_co = (df['Close'] / df['Open']).apply(np.log)\n",
    "        log_oc = (df['Open'] / df['Close'].shift(1)).apply(np.log)\n",
    "        log_cc = (df['Close'] / df['Close'].shift(1)).apply(np.log)\n",
    "        \n",
    "        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n",
    "        close_vol = log_cc.rolling(window=window).var()\n",
    "        open_vol = log_oc.rolling(window=window).var()\n",
    "        window_rs = rs.rolling(window=window).mean()\n",
    "\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        return np.sqrt(open_vol + k * window_rs)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    @staticmethod\n",
    "    def triple_barrier_labels(prices, vol, pt=1.0, sl=1.0, barrier_window=10):\n",
    "        \"\"\"\n",
    "        Implements the Triple Barrier Method.\n",
    "        Labels: 1 (Profit Target Hit), -1 (Stop Loss Hit), 0 (Time Limit/Neutral)\n",
    "        \"\"\"\n",
    "        labels = pd.Series(0, index=prices.index)\n",
    "        # Shift prices to align future outcome with current row\n",
    "        # However, to avoid look-ahead in features, we usually compute label for row t based on t+1...t+k\n",
    "        # This function generates the TARGET variable (y) for training.\n",
    "        \n",
    "        limit = len(prices) - barrier_window\n",
    "        p_values = prices.values\n",
    "        v_values = vol.values\n",
    "        \n",
    "        for i in range(limit):\n",
    "            current_p = p_values[i]\n",
    "            current_vol = v_values[i]\n",
    "            \n",
    "            # Dynamic barriers based on volatility [cite: 215]\n",
    "            target = current_p * (1 + pt * current_vol)\n",
    "            stop = current_p * (1 - sl * current_vol)\n",
    "            \n",
    "            future_window = p_values[i+1 : i+1+barrier_window]\n",
    "            \n",
    "            hit_target = np.where(future_window >= target)[0]\n",
    "            hit_stop = np.where(future_window <= stop)[0]\n",
    "            \n",
    "            first_target = hit_target[0] if len(hit_target) > 0 else barrier_window + 1\n",
    "            first_stop = hit_stop[0] if len(hit_stop) > 0 else barrier_window + 1\n",
    "            \n",
    "            if first_target < first_stop and first_target <= barrier_window:\n",
    "                labels.iloc[i] = 1\n",
    "            elif first_stop < first_target and first_stop <= barrier_window:\n",
    "                labels.iloc[i] = 0 # In Meta-Labeling, we often treat Stop (-1) as 0 (Do Not Trade)\n",
    "            # Else 0 (Time limit reached or neutral)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381033d7",
   "metadata": {},
   "source": [
    "## Base Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5398e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseStrategy(ABC):\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.data = None\n",
    "        self.results = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\")\n",
    "        warmup_start_dt = start_dt - timedelta(days=warmup_years*365)\n",
    "        warmup_start_str = warmup_start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            self.data = yf.download(self.ticker, start=warmup_start_str, end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(self.data.columns, pd.MultiIndex): \n",
    "                self.data.columns = self.data.columns.get_level_values(0)\n",
    "            if 'Adj Close' not in self.data.columns: \n",
    "                self.data['Adj Close'] = self.data['Close']\n",
    "            self.data['Returns'] = self.data['Adj Close'].pct_change()\n",
    "            self.data.dropna(inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {self.ticker}: {e}\")\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_signals(self):\n",
    "        pass\n",
    "\n",
    "    def run_backtest(self, transaction_cost=0.0005, rebalance_threshold=0.1):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        \n",
    "        backtest_mask = self.data.index >= self.start_date\n",
    "        df = self.data.loc[backtest_mask].copy()\n",
    "        if df.empty: return\n",
    "\n",
    "        # Position Smoothing\n",
    "        clean_positions = []\n",
    "        current_pos = 0.0\n",
    "        raw_signals = df['Signal'].values\n",
    "        \n",
    "        for target in raw_signals:\n",
    "            if abs(target - current_pos) > rebalance_threshold:\n",
    "                current_pos = target\n",
    "            clean_positions.append(current_pos)\n",
    "            \n",
    "        df['Position'] = clean_positions\n",
    "        df['Prev_Position'] = df['Position'].shift(1).fillna(0)\n",
    "        df['Turnover'] = (df['Prev_Position'] - df['Position'].shift(2).fillna(0)).abs()\n",
    "        df['Gross_Returns'] = df['Prev_Position'] * df['Returns']\n",
    "        df['Net_Returns'] = df['Gross_Returns'] - (df['Turnover'] * transaction_cost)\n",
    "        df['Net_Returns'].fillna(0, inplace=True)\n",
    "        \n",
    "        df['Cumulative_Strategy'] = (1 + df['Net_Returns']).cumprod()\n",
    "        df['Cumulative_Market'] = (1 + df['Returns']).cumprod()\n",
    "        \n",
    "        roll_max = df['Cumulative_Strategy'].cummax()\n",
    "        df['Drawdown'] = (df['Cumulative_Strategy'] / roll_max) - 1.0\n",
    "        \n",
    "        self.results = df\n",
    "        \n",
    "        # Performance Calculation\n",
    "        total_ret = df['Cumulative_Strategy'].iloc[-1] - 1\n",
    "        vol = df['Net_Returns'].std() * np.sqrt(252)\n",
    "        sharpe = (df['Net_Returns'].mean() / df['Net_Returns'].std()) * np.sqrt(252) if vol > 0 else 0\n",
    "        max_dd = df['Drawdown'].min()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'Total Return': total_ret,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Max Drawdown': max_dd\n",
    "        }\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cee99",
   "metadata": {},
   "source": [
    "## Model From NB V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0ccb4",
   "metadata": {},
   "source": [
    "### V1_Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93a7c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV1_Baseline(BaseStrategy):\n",
    "    \"\"\"V1: Fixed FracDiff, Standard GMM.\"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "        df['Cluster'] = gmm.fit_predict(X_scaled)\n",
    "        \n",
    "        stats = df.groupby('Cluster')['Returns_Smoothed'].mean().sort_values().index\n",
    "        mapping = {stats[0]: -1, stats[1]: 0, stats[2]: 1}\n",
    "        df['Regime'] = df['Cluster'].map(mapping)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['Regime'] == 1) & (df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[(df['Regime'] == 0) & (df['RSI'] < 40), 'Signal'] = 1\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Vol_Scaler'] = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        df['Signal'] = df['Signal'] * df['Vol_Scaler']\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529bcb35",
   "metadata": {},
   "source": [
    "### V3_Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9445384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV3_Macro(BaseStrategy):\n",
    "    \"\"\"V3: Macro (SPY) Filter.\"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            self.spy_data = spy[['Adj Close']].rename(columns={'Adj Close': 'SPY_Price'})\n",
    "        except: pass\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left')\n",
    "            df['SPY_MA200'] = df['SPY_Price'].rolling(window=200).mean()\n",
    "            df['Macro_Bull'] = df['SPY_Price'] > df['SPY_MA200']\n",
    "        else:\n",
    "            df['Macro_Bull'] = True\n",
    "            \n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Adj Close'].apply(np.log), d=0.4, window=50)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        df['Signal'] = 0\n",
    "        df.loc[(df['FracDiff'] > 0), 'Signal'] = 1\n",
    "        df.loc[df['Macro_Bull'] == False, 'Signal'] = 0\n",
    "        \n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        df['Signal'] = df['Signal'] * (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77a15c",
   "metadata": {},
   "source": [
    "### V9_Unshackled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e2ad3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV9_RegimeUnshackled(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V9 (Robust): The 'Unshackled' Regime Model with Walk-Forward Learning.\n",
    "    \n",
    "    CRITICAL FIX:\n",
    "    - Replaces static gmm.fit_predict() with a Rolling Walk-Forward loop.\n",
    "    - Eliminates Look-Ahead Bias by retraining the regime model every month\n",
    "      using only the trailing 1-year window.\n",
    "    - Solves the 'Label Switching' problem dynamically at each step.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.spy_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        try:\n",
    "            # Fetch Macro Context (SPY) for the 'Alpha Clause'\n",
    "            spy = yf.download(\"SPY\", start=start_dt.strftime(\"%Y-%m-%d\"), end=self.end_date, progress=False, auto_adjust=False)\n",
    "            if isinstance(spy.columns, pd.MultiIndex): spy.columns = spy.columns.get_level_values(0)\n",
    "            \n",
    "            # Simple Macro Trend (200 SMA)\n",
    "            spy['Macro_Trend'] = (spy['Adj Close'] > spy['Adj Close'].rolling(200).mean()).astype(int)\n",
    "            self.spy_data = spy[['Macro_Trend']]\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    def _apply_kalman_filter(self, prices):\n",
    "        # [cite: 151] Kalman Filter for recursive state estimation\n",
    "        x = prices.values\n",
    "        n = len(x)\n",
    "        state = np.zeros(n)\n",
    "        slope = np.zeros(n)\n",
    "        state[0] = x[0]\n",
    "        P, Q, R = 1.0, 0.001, 0.1 # Static params for robustness\n",
    "        \n",
    "        for t in range(1, n):\n",
    "            pred_state = state[t-1] + slope[t-1]\n",
    "            pred_P = P + Q\n",
    "            measurement = x[t]\n",
    "            residual = measurement - pred_state\n",
    "            \n",
    "            K = pred_P / (pred_P + R)\n",
    "            state[t] = pred_state + K * residual\n",
    "            slope[t] = 0.9 * slope[t-1] + 0.1 * (state[t] - state[t-1])\n",
    "            P = (1 - K) * pred_P\n",
    "            \n",
    "        return pd.Series(slope, index=prices.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- 1. Feature Engineering ---\n",
    "        if self.spy_data is not None:\n",
    "            df = df.join(self.spy_data, how='left').fillna(method='ffill')\n",
    "        else: \n",
    "            df['Macro_Trend'] = 1\n",
    "            \n",
    "        # Volatility & Kalman Slope [cite: 82, 151]\n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['Kalman_Slope'] = self._apply_kalman_filter(np.log(df['Adj Close']))\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        \n",
    "        # Features for GMM (Smoothed to reduce noise)\n",
    "        df['Returns_Smoothed'] = df['Returns'].rolling(5).mean()\n",
    "        df['Vol_Smoothed'] = df['Volatility'].rolling(5).mean()\n",
    "        \n",
    "        # Drop NaN from warmup\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- 2. Rolling Walk-Forward GMM ---\n",
    "        # We need to predict 'Regime_Type' for every row without looking ahead.\n",
    "        \n",
    "        train_window = 252  # Lookback: 1 Year\n",
    "        refit_step = 21     # Re-train every Month\n",
    "        \n",
    "        # Initialize default regime (CHOP)\n",
    "        df['Regime_Type'] = 'CHOP' \n",
    "        \n",
    "        # Prepare Feature Matrix\n",
    "        X = df[['Returns_Smoothed', 'Vol_Smoothed']].values\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        indices = np.arange(len(df))\n",
    "        \n",
    "        # Start loop after the first training window\n",
    "        for t in range(train_window, len(df), refit_step):\n",
    "            start_idx = t - train_window\n",
    "            end_idx = t\n",
    "            predict_end_idx = min(t + refit_step, len(df))\n",
    "            \n",
    "            # A. Train on PAST window\n",
    "            X_train = X[start_idx:end_idx]\n",
    "            \n",
    "            # Scale locally (Standardization must also be walk-forward)\n",
    "            #  \"Standard algorithms assume training data... same probability distribution\"\n",
    "            scaler_local = StandardScaler()\n",
    "            X_train_scaled = scaler_local.fit_transform(X_train)\n",
    "            \n",
    "            try:\n",
    "                # Fit GMM\n",
    "                gmm = GaussianMixture(n_components=3, random_state=42, n_init=5)\n",
    "                gmm.fit(X_train_scaled)\n",
    "                \n",
    "                # B. Predict on NEXT window (Future)\n",
    "                X_future = X[end_idx:predict_end_idx]\n",
    "                X_future_scaled = scaler_local.transform(X_future)\n",
    "                clusters_future = gmm.predict(X_future_scaled)\n",
    "                \n",
    "                # C. Dynamic Label Mapping (Solve Label Switching)\n",
    "                # We map clusters to Bull/Bear based on the *Training* means\n",
    "                # Calculate mean return for each cluster center in the training set\n",
    "                # We can approximate this by predicting the training set again\n",
    "                train_clusters = gmm.predict(X_train_scaled)\n",
    "                \n",
    "                # Create a temp dataframe to sort clusters\n",
    "                temp_df = pd.DataFrame({\n",
    "                    'Ret': X_train[:, 0], # Returns is column 0\n",
    "                    'Cluster': train_clusters\n",
    "                })\n",
    "                \n",
    "                stats = temp_df.groupby('Cluster')['Ret'].mean().sort_values()\n",
    "                \n",
    "                # The cluster with lowest mean return = BEAR\n",
    "                # The cluster with highest mean return = BULL\n",
    "                # Middle = CHOP\n",
    "                if len(stats) == 3:\n",
    "                    bear_c = stats.index[0]\n",
    "                    chop_c = stats.index[1]\n",
    "                    bull_c = stats.index[2]\n",
    "                    \n",
    "                    mapping = {bear_c: 'BEAR', chop_c: 'CHOP', bull_c: 'BULL'}\n",
    "                else:\n",
    "                    # Fallback if GMM collapses to fewer clusters\n",
    "                    mapping = {c: 'CHOP' for c in stats.index}\n",
    "\n",
    "                # Apply mapping to the prediction window\n",
    "                regimes_mapped = [mapping.get(c, 'CHOP') for c in clusters_future]\n",
    "                \n",
    "                # Store results in the main DataFrame\n",
    "                # Use iloc for integer-based indexing on the slice\n",
    "                df.iloc[end_idx:predict_end_idx, df.columns.get_loc('Regime_Type')] = regimes_mapped\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Keep default 'CHOP' on failure\n",
    "                pass\n",
    "\n",
    "        # --- 3. Unshackled Signal Logic (unchanged from V9) ---\n",
    "        df['Signal'] = 0.0\n",
    "        \n",
    "        # A. BULL REGIME: \"Trust The Trend\"\n",
    "        bull_signal = (df['Regime_Type'] == 'BULL')\n",
    "        df.loc[bull_signal, 'Signal'] = 1.0\n",
    "        \n",
    "        # Boost: If Kalman agrees (Strong Trend), increase leverage\n",
    "        strong_trend = bull_signal & (df['Kalman_Slope'] > 0)\n",
    "        df.loc[strong_trend, 'Signal'] = 1.3\n",
    "        \n",
    "        # B. CHOP REGIME: \"Buy Dips\"\n",
    "        # chop_buy = (df['Regime_Type'] == 'CHOP') & (df['RSI'] < 45)\n",
    "        # df.loc[chop_buy, 'Signal'] = 1.0\n",
    "\n",
    "        # 1. Calculate a \"Floor\" (e.g., Lower Bollinger Band)\n",
    "        df['SMA_20'] = df['Adj Close'].rolling(20).mean()\n",
    "        df['BB_Lower'] = df['SMA_20'] - 2 * df['SMA_20'].rolling(20).std()\n",
    "\n",
    "        # 2. Strict Chop Entry\n",
    "        # OLD: RSI < 45\n",
    "        # NEW: RSI < 45 AND Price > BB_Lower (Don't buy if it's crashing through the floor)\n",
    "        #      AND FracDiff > -0.1 (Don't buy if trend is catastrophically negative)\n",
    "\n",
    "        chop_buy = (\n",
    "            (df['Regime_Type'] == 'CHOP') & \n",
    "            (df['RSI'] < 45) & \n",
    "            (df['Adj Close'] > df['BB_Lower']) # The Falling Knife Filter\n",
    "        )\n",
    "        df.loc[chop_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # C. BEAR REGIME: \"Survival\" (Cash unless deep panic)\n",
    "        panic_buy = (df['Regime_Type'] == 'BEAR') & (df['RSI'] < 25)\n",
    "        df.loc[panic_buy, 'Signal'] = 1.0\n",
    "        \n",
    "        # --- 4. The Alpha Clause (Macro Handling) ---\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        \n",
    "        # Volatility Sizing (Risk Parity) [cite: 254]\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        \n",
    "        # Macro Filter: Halve size if SPY is bearish, but don't exit fully (Alpha Clause)\n",
    "        macro_scaler = df['Macro_Trend'].map({1: 1.0, 0: 0.5})\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler * macro_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f7d61",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80c0270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble(BaseStrategy):\n",
    "    \"\"\"\n",
    "    The 'All-Weather' Ensemble.\n",
    "    \n",
    "    Combines V3 (Macro Trend) and V9 (Regime Unshackled) into a single\n",
    "    portfolio-level signal.\n",
    "    \n",
    "    Logic:\n",
    "    1. Runs V3 to capture high-beta trends (NVDA, Bitcoin).\n",
    "    2. Runs V9 to capture regime-based alpha and protect downside (JPM, BABA).\n",
    "    3. Blends signals using a 'Correlation-Adjusted' weighting or fixed 50/50.\n",
    "    4. Applies a final Volatility Target to the combined equity curve to ensure\n",
    "       the two strategies don't stack up to dangerous leverage.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date, w_v3=0.5, w_v9=0.5):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.w_v3 = w_v3\n",
    "        self.w_v9 = w_v9\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # Fetch once for efficiency (logic could be optimized to share DF, \n",
    "        # but separate fetch ensures cleaner encapsulation)\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # We share the index/data from one of them for the main wrapper\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Align Indices (Inner Join to be safe)\n",
    "        df = self.data.copy()\n",
    "        s3 = self.strat_v3.data['Signal']\n",
    "        s9 = self.strat_v9.data['Signal']\n",
    "        \n",
    "        # Merge signals into main DF\n",
    "        df['Sig_V3'] = s3\n",
    "        df['Sig_V9'] = s9\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. The Allocation Logic\n",
    "        # Default: Fixed Weight (Core-Satellite Approach)\n",
    "        # V3 (Beta) + V9 (Alpha)\n",
    "        \n",
    "        # We blend the RAW signals.\n",
    "        # Note: Signals are already Vol-Targeted to ~15% inside sub-classes.\n",
    "        # Simple addition would double vol if correlation=1.\n",
    "        raw_blend = (df['Sig_V3'] * self.w_v3) + (df['Sig_V9'] * self.w_v9)\n",
    "        \n",
    "        # 3. Ensemble Volatility Control\n",
    "        # If V3 and V9 agree (both Long), we get high exposure.\n",
    "        # If they disagree (V3 Long, V9 Cash), we get half exposure.\n",
    "        # This naturally deleverages during uncertainty.\n",
    "        \n",
    "        df['Signal'] = raw_blend\n",
    "        \n",
    "        # Optional: Re-Target Volatility of the *Ensemble*\n",
    "        # (Prevents leverage creep if strategies are highly correlated)\n",
    "        # For now, we trust the weighted sum to act as a diversification benefit.\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b5a46",
   "metadata": {},
   "source": [
    "### Adaptive Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd66f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble_Adaptive(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V10: The Adaptive Ensemble (Dynamic Weighting).\n",
    "    \n",
    "    Instead of fixed weights, this strategy re-allocates capital quarterly \n",
    "    based on the recent Risk-Adjusted Performance (Sharpe) of the sub-strategies.\n",
    "    \n",
    "    Logic:\n",
    "    1. Lookback: 126 Days (6 Months).\n",
    "    2. Rebalance: Every 63 Days (Quarterly).\n",
    "    3. Weighting:\n",
    "       - Calculate Sharpe Ratio for V3 and V9 in the lookback window.\n",
    "       - If Sharpe > 0: Weight is proportional to Sharpe.\n",
    "       - If Sharpe < 0: Weight is set to 0.\n",
    "       - Normalize weights to sum to 1.0.\n",
    "       \n",
    "    This allows the portfolio to automatically 'Risk On' into V3 during strong bulls\n",
    "    and 'Risk Off' into V9 during bears/chop.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        # Instantiate sub-strategies\n",
    "        self.strat_v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.strat_v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        self.strat_v3.fetch_data(warmup_years)\n",
    "        self.strat_v9.fetch_data(warmup_years)\n",
    "        \n",
    "        # Use one of the dataframes as the base\n",
    "        if self.strat_v3.data is not None and not self.strat_v3.data.empty:\n",
    "            self.data = self.strat_v3.data.copy()\n",
    "        elif self.strat_v9.data is not None:\n",
    "            self.data = self.strat_v9.data.copy()\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.strat_v3.data is None or self.strat_v9.data is None: return\n",
    "        \n",
    "        # 1. Generate Sub-Signals\n",
    "        self.strat_v3.generate_signals()\n",
    "        self.strat_v9.generate_signals()\n",
    "        \n",
    "        # Merge Data\n",
    "        df = self.data.copy()\n",
    "        df['Sig_V3'] = self.strat_v3.data['Signal']\n",
    "        df['Sig_V9'] = self.strat_v9.data['Signal']\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 2. Simulate Sub-Strategy Returns (for metric calculation)\n",
    "        # We need to know how they *would* have performed to weight them.\n",
    "        # Lag signals by 1 to avoid lookahead bias when calculating returns.\n",
    "        df['Ret_V3'] = df['Sig_V3'].shift(1) * df['Returns']\n",
    "        df['Ret_V9'] = df['Sig_V9'].shift(1) * df['Returns']\n",
    "        \n",
    "        # 3. Walk-Forward Weight Optimization\n",
    "        df['W_V3'] = 0.5 # Default start\n",
    "        df['W_V9'] = 0.5\n",
    "        \n",
    "        lookback = 126      # 6 Months Lookback\n",
    "        rebalance_freq = 21 # Monthly Rebalance (Faster adaptation)\n",
    "        \n",
    "        indices = df.index\n",
    "        \n",
    "        if len(df) > lookback:\n",
    "            for t in range(lookback, len(df), rebalance_freq):\n",
    "                train_start = indices[t - lookback]\n",
    "                train_end = indices[t]\n",
    "                test_end_idx = min(t + rebalance_freq, len(df))\n",
    "                test_end = indices[test_end_idx - 1]\n",
    "                \n",
    "                # Calculate Sharpe in Lookback Window\n",
    "                # Add small epsilon to std to avoid division by zero\n",
    "                v3_mean = df.loc[train_start:train_end, 'Ret_V3'].mean()\n",
    "                v3_std = df.loc[train_start:train_end, 'Ret_V3'].std() + 1e-9\n",
    "                sharpe_v3 = (v3_mean / v3_std) * np.sqrt(252)\n",
    "                \n",
    "                v9_mean = df.loc[train_start:train_end, 'Ret_V9'].mean()\n",
    "                v9_std = df.loc[train_start:train_end, 'Ret_V9'].std() + 1e-9\n",
    "                sharpe_v9 = (v9_mean / v9_std) * np.sqrt(252)\n",
    "                \n",
    "                # Weighting Logic\n",
    "                # 1. Filter: If Sharpe is negative, set score to 0\n",
    "                score_v3 = max(0, sharpe_v3)\n",
    "                score_v9 = max(0, sharpe_v9)\n",
    "                \n",
    "                # 2. Normalize\n",
    "                total_score = score_v3 + score_v9\n",
    "                \n",
    "                if total_score > 0:\n",
    "                    w_v3 = score_v3 / total_score\n",
    "                    w_v9 = score_v9 / total_score\n",
    "                else:\n",
    "                    # Both are failing? Default to Defensive (V9) or Cash (0)\n",
    "                    # Let's default to V9 (Safety) as the 'bunker'\n",
    "                    w_v3 = 0.0\n",
    "                    w_v9 = 1.0\n",
    "                \n",
    "                # Apply weights to NEXT window\n",
    "                df.loc[train_end:test_end, 'W_V3'] = w_v3\n",
    "                df.loc[train_end:test_end, 'W_V9'] = w_v9\n",
    "                \n",
    "        # 4. Final Signal Generation\n",
    "        # Blend the signals using the dynamic weights\n",
    "        df['Signal'] = (df['Sig_V3'] * df['W_V3']) + (df['Sig_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ec1cd",
   "metadata": {},
   "source": [
    "### V12_Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5892e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV12_Macro_Switch(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V12: The Macro-Guided Ensemble.\n",
    "    \n",
    "    Replaces lookback windows with Real-Time Economic Data.\n",
    "    \n",
    "    DATA SOURCES (Yahoo Finance):\n",
    "    1. ^VIX: CBOE Volatility Index.\n",
    "    2. ^TNX: 10-Year Treasury Yield.\n",
    "    \n",
    "    LOGIC:\n",
    "    1. Calculate 'Macro Stress Score' (0.0 to 1.0).\n",
    "       - VIX Component: Normalized against recent history. High VIX = High Stress.\n",
    "       - Yield Component: Rate of Change (ROC) of TNX. Spiking rates = High Stress.\n",
    "    \n",
    "    2. Dynamic Weighting:\n",
    "       - Weight_V3 (Trend) = 1.0 - Stress_Score\n",
    "       - Weight_V9 (Safety) = Stress_Score\n",
    "       \n",
    "    HYPOTHESIS:\n",
    "    VIX and Rates often spike BEFORE the price crash is fully realized. \n",
    "    This allows the model to switch to safety faster than a Moving Average.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v3 = StrategyV3_Macro(ticker, start_date, end_date)\n",
    "        self.v9 = StrategyV9_RegimeUnshackled(ticker, start_date, end_date)\n",
    "        # We store macro data separately\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # 1. Fetch Ticker Data\n",
    "        self.v3.fetch_data(warmup_years)\n",
    "        self.v9.fetch_data(warmup_years)\n",
    "        \n",
    "        if self.v3.data is None or self.v3.data.empty: return\n",
    "        self.data = self.v3.data.copy()\n",
    "        \n",
    "        # 2. Fetch Macro Data (VIX and TNX)\n",
    "        start_dt = (self.data.index[0] - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "        end_dt = self.end_date\n",
    "        \n",
    "        try:\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=end_dt, progress=False, auto_adjust=True)\n",
    "            \n",
    "            # Cleaning\n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            macro_df = pd.DataFrame(index=self.data.index)\n",
    "            # Align macro data to the ticker's trading days (ffill for holidays)\n",
    "            macro_df['VIX'] = vix['Close'].reindex(self.data.index, method='ffill')\n",
    "            macro_df['TNX'] = tnx['Close'].reindex(self.data.index, method='ffill')\n",
    "            \n",
    "            self.macro_data = macro_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Macro Data Fetch Error: {e}\")\n",
    "            # Fallback: Zero stress\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        \n",
    "        # 1. Run Sub-Strategies\n",
    "        self.v3.generate_signals()\n",
    "        self.v9.generate_signals()\n",
    "        \n",
    "        # 2. Sync Data\n",
    "        df = self.data.copy()\n",
    "        df = df.join(self.v3.data[['Signal']].rename(columns={'Signal':'S_V3'}), how='left')\n",
    "        df = df.join(self.v9.data[['Signal']].rename(columns={'Signal':'S_V9'}), how='left')\n",
    "        \n",
    "        # 3. Calculate Macro Stress Score\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # A. VIX Stress (Fear)\n",
    "        # Normalize VIX: If VIX > 30, Stress = 1.0. If VIX < 15, Stress = 0.0.\n",
    "        # Uses a rolling Z-score or simple clamp? Simple clamp is more robust to regime shifts.\n",
    "        macro['VIX_Stress'] = ((macro['VIX'] - 15) / (30 - 15)).clip(0, 1)\n",
    "        \n",
    "        # B. Yield Stress (Rate Shock)\n",
    "        # We care about SPEED of rate rise, not just level.\n",
    "        # Calculate 20-day Rate of Change of TNX\n",
    "        macro['TNX_ROC'] = macro['TNX'].pct_change(20)\n",
    "        # If Yields rise > 10% in a month, that's a shock.\n",
    "        macro['TNX_Stress'] = (macro['TNX_ROC'] / 0.10).clip(0, 1)\n",
    "        \n",
    "        # Combined Stress (Max of either Fear or Rate Shock)\n",
    "        # We use Max because either one can crash the market independently.\n",
    "        macro['Total_Stress'] = macro[['VIX_Stress', 'TNX_Stress']].max(axis=1)\n",
    "        \n",
    "        # 4. Allocate Weights\n",
    "        # Smooth the stress signal to avoid daily jitter (3-day avg)\n",
    "        stress_signal = macro['Total_Stress'].rolling(3).mean().fillna(0)\n",
    "        \n",
    "        df['W_V9'] = stress_signal        # High Stress -> More Safety\n",
    "        df['W_V3'] = 1.0 - stress_signal  # Low Stress -> More Trend\n",
    "        \n",
    "        # 5. Final Signal\n",
    "        df['Signal'] = (df['S_V3'] * df['W_V3']) + (df['S_V9'] * df['W_V9'])\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a54ccc",
   "metadata": {},
   "source": [
    "### V20_MacroDom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9019c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV20_MacroDominance(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V20: The 'Macro Dominance' Strategy.\n",
    "    \n",
    "    This is the optimized single-model architecture.\n",
    "    1. CORE ENGINE: V12 (Macro Switch). Uses VIX/TNX to drive risk on/off.\n",
    "    2. SECTOR PATCH: Inverts Macro logic for Energy/Commodities (Inflation Trade).\n",
    "    3. SAFETY VETO: Uses V9's 'Trend Floor' to kill trades if Price < Bollinger Low.\n",
    "    \n",
    "    No averaging. No ensembles. Pure Signal.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        # We use V12 logic natively, but need V9-style volatility calcs for the Floor\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        \n",
    "        # 1. Fetch Macro Data\n",
    "        try:\n",
    "            start_dt = self.data.index[0]\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            \n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            self.macro_data = pd.DataFrame(index=self.data.index)\n",
    "            self.macro_data['VIX'] = vix['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "            self.macro_data['TNX'] = tnx['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "        except:\n",
    "            print(\"Macro Fetch Failed. Defaulting to Neutral.\")\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4.0}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        df = self.data.copy()\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # --- 1. The V12 Macro Engine ---\n",
    "        # Stress Score Calculation\n",
    "        macro['VIX_Stress'] = ((macro['VIX'] - 15) / 15).clip(0, 1)\n",
    "        macro['TNX_ROC'] = macro['TNX'].pct_change(20) # 1-Month Rate Shock\n",
    "        macro['TNX_Stress'] = (macro['TNX_ROC'] / 0.10).clip(0, 1)\n",
    "        \n",
    "        # Combined Stress (Fear OR Rates)\n",
    "        macro['Total_Stress'] = macro[['VIX_Stress', 'TNX_Stress']].max(axis=1).rolling(3).mean().fillna(0)\n",
    "        \n",
    "        # Base Signal: 1.0 - Stress (Calm = Buy, Panic = Sell)\n",
    "        df['Signal'] = 1.0 - macro['Total_Stress']\n",
    "        \n",
    "        # --- 2. The Sector Patch (XLE Fix) ---\n",
    "        # If Ticker is Energy, Invert the logic: Stress (Inflation) is GOOD.\n",
    "        # Hard-coded list for robustness.\n",
    "        energy_tickers = [\"XLE\", \"XOM\", \"CVX\", \"OIH\", \"USO\", \"GLD\"]\n",
    "        if self.ticker in energy_tickers:\n",
    "            # Logic: If Stress is high (1.0), Signal becomes 1.0. \n",
    "            # If Stress is low (0.0), Signal stays 1.0 (Energy also works in calm bulls).\n",
    "            # We essentially IGNORE stress selling, and maybe Boost on stress.\n",
    "            # Simple Fix: Signal = MAX(Signal, Total_Stress)\n",
    "            # If Stress is 0.8, Base Signal is 0.2. Max is 0.8. We Buy.\n",
    "            df['Signal'] = np.maximum(df['Signal'], macro['Total_Stress'])\n",
    "            \n",
    "        # --- 3. The V9 Safety Veto (Trend Floor) ---\n",
    "        # Prevent buying falling knives (BABA fix).\n",
    "        # Logic: If Price < Lower Bollinger Band, Signal = 0 (Hard Stop).\n",
    "        df['SMA_20'] = df['Adj Close'].rolling(20).mean()\n",
    "        df['BB_Lower'] = df['SMA_20'] - 2 * df['SMA_20'].rolling(20).std()\n",
    "        \n",
    "        # Allow a tiny buffer (0.98) to avoid noise stop-outs\n",
    "        is_crashing = df['Adj Close'] < (df['BB_Lower'] * 0.98)\n",
    "        \n",
    "        # Override Signal\n",
    "        df.loc[is_crashing, 'Signal'] = 0.0\n",
    "        \n",
    "        # --- 4. Volatility Sizing ---\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / FeatureLab.yang_zhang_volatility(df)).clip(upper=1.5)\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222544b",
   "metadata": {},
   "source": [
    "### V14_AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6d96e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- 1. The Transformer Architecture ---\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Encoder for Time-Series Classification.\n",
    "    Input: (Batch_Size, Seq_Len, Num_Features)\n",
    "    Output: (Batch_Size, Num_Classes) -> Probabilities for [-1, 0, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, num_classes=3, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        \n",
    "        # Project features to d_model dimension\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer Encoder Layer (The \"Brain\")\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Final Classification Head\n",
    "        self.decoder = nn.Linear(d_model, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch, seq_len, features]\n",
    "        src = self.embedding(src)      # -> [batch, seq_len, d_model]\n",
    "        src = self.pos_encoder(src)    # Add positional context\n",
    "        output = self.transformer_encoder(src) # -> [batch, seq_len, d_model]\n",
    "        \n",
    "        # We take the output of the *last* time step for classification\n",
    "        last_step_output = output[:, -1, :] \n",
    "        \n",
    "        prediction = self.decoder(last_step_output)\n",
    "        return self.softmax(prediction)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects information about the relative or absolute position of the tokens.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- 2. The Strategy Wrapper ---\n",
    "class StrategyV14_Transformer(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V14.1: Continuous Learning Transformer.\n",
    "    \n",
    "    CRITICAL CHANGE: \n",
    "    The model is initialized ONCE before the loop. \n",
    "    It 'remembers' previous regimes and fine-tunes on new data \n",
    "    rather than restarting from scratch.\n",
    "    \"\"\"\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # --- Feature Engineering ---\n",
    "        # 1. Stationarity & Normalization\n",
    "        df['Log_Price'] = df['Adj Close'].apply(np.log)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Log_Price'], d=0.4, window=30)\n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close']) / 100.0 # Normalize RSI to 0-1\n",
    "        \n",
    "        # 2. Triple Barrier Targets\n",
    "        # We widen the barriers slightly to reduce noise for the DL model\n",
    "        raw_labels = FeatureLab.triple_barrier_labels(\n",
    "            df['Adj Close'], df['Volatility'], pt=2.0, sl=2.0, barrier_window=10\n",
    "        )\n",
    "        # Map: -1(Sell)->0, 0(Neutral)->1, 1(Buy)->2\n",
    "        label_map = {-1: 0, 0: 1, 1: 2}\n",
    "        df['Target'] = raw_labels.map(label_map)\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- Tensor Prep ---\n",
    "        SEQ_LEN = 30\n",
    "        feature_cols = ['FracDiff', 'Volatility', 'RSI']\n",
    "        \n",
    "        # Robust Scaling (Median/IQR) to handle outliers better than Standard\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "        \n",
    "        data_matrix = df[feature_cols].values\n",
    "        targets = df['Target'].values\n",
    "        \n",
    "        X_seq, y_seq, indices = [], [], []\n",
    "        for i in range(len(data_matrix) - SEQ_LEN):\n",
    "            X_seq.append(data_matrix[i : i+SEQ_LEN])\n",
    "            y_seq.append(targets[i + SEQ_LEN - 1])\n",
    "            indices.append(df.index[i + SEQ_LEN - 1])\n",
    "            \n",
    "        X_tensor = torch.FloatTensor(np.array(X_seq))\n",
    "        y_tensor = torch.LongTensor(np.array(y_seq))\n",
    "        \n",
    "        # --- Continuous Walk-Forward Loop ---\n",
    "        train_window = 500\n",
    "        test_window = 21\n",
    "        signals = pd.Series(0, index=df.index)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\"))\n",
    "        print(f\"Training V14.1 (Continuous) on {device}...\")\n",
    "        \n",
    "        # INITIALIZE ONCE (The Fix)\n",
    "        model = TimeSeriesTransformer(\n",
    "            input_dim=len(feature_cols), \n",
    "            d_model=32, \n",
    "            nhead=2, \n",
    "            num_layers=1, \n",
    "            num_classes=3,\n",
    "            dropout=0.2 # Higher dropout for regularization\n",
    "        ).to(device)\n",
    "        \n",
    "        # Lower LR for fine-tuning stability\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        # Weighted Loss to combat class imbalance (Neutral is common)\n",
    "        # Penalize missing a Bull/Bear move heavily\n",
    "        class_weights = torch.tensor([2.0, 1.0, 2.0]).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # Rolling Window\n",
    "        for t in range(train_window, len(X_tensor), test_window):\n",
    "            # Define Windows\n",
    "            # Train on trailing window\n",
    "            X_train = X_tensor[t-train_window : t].to(device)\n",
    "            y_train = y_tensor[t-train_window : t].to(device)\n",
    "            \n",
    "            # Predict next month\n",
    "            X_test = X_tensor[t : min(t+test_window, len(X_tensor))].to(device)\n",
    "            if len(X_test) == 0: break\n",
    "            \n",
    "            # --- Fine-Tuning Step ---\n",
    "            model.train()\n",
    "            # We train for fewer epochs per step since we retain knowledge\n",
    "            # But we ensure we don't overfit the recent noise\n",
    "            for _ in range(5): \n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_train)\n",
    "                loss = criterion(output, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            # --- Inference Step ---\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probs = model(X_test)\n",
    "                \n",
    "                # Probability Thresholding\n",
    "                # We need high conviction to trade\n",
    "                buy_probs = probs[:, 2].cpu().numpy()\n",
    "                sell_probs = probs[:, 0].cpu().numpy()\n",
    "                \n",
    "                batch_signals = np.zeros(len(X_test))\n",
    "                \n",
    "                # Dynamic Threshold: Only trade if model is > 50% sure\n",
    "                batch_signals[buy_probs > 0.5] = 1\n",
    "                batch_signals[sell_probs > 0.5] = 0 # Cash/Short\n",
    "                \n",
    "                # Map to DataFrame\n",
    "                batch_idx = indices[t : min(t+test_window, len(X_tensor))]\n",
    "                signals.loc[batch_idx] = batch_signals\n",
    "\n",
    "        df['Signal'] = signals\n",
    "        \n",
    "        # Volatility Sizing (Safety Net)\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        \n",
    "        self.data = df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c3280",
   "metadata": {},
   "source": [
    "### V15_MacroTransf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1a707a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV15_MacroTransformer(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V15: The 'Context-Aware' Transformer.\n",
    "    \n",
    "    Fusion of V20 (Macro Data) and V14 (Deep Learning).\n",
    "    Input Features: [FracDiff, Volatility, RSI, VIX_Norm, TNX_ROC]\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        \n",
    "        # 1. Fetch Macro Data (Same logic as V20)\n",
    "        try:\n",
    "            start_dt = self.data.index[0]\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            \n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            self.macro_data = pd.DataFrame(index=self.data.index)\n",
    "            self.macro_data['VIX'] = vix['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "            self.macro_data['TNX'] = tnx['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "        except:\n",
    "            print(\"Macro Fetch Failed. Defaulting.\")\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4.0}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.data.empty: return\n",
    "        df = self.data.copy()\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # --- Feature Engineering ---\n",
    "        # 1. Technicals\n",
    "        df['Log_Price'] = df['Adj Close'].apply(np.log)\n",
    "        df['FracDiff'] = FeatureLab.frac_diff_fixed(df['Log_Price'], d=0.4, window=30)\n",
    "        df['Volatility'] = FeatureLab.yang_zhang_volatility(df)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close']) / 100.0\n",
    "        \n",
    "        # 2. Macro (The New Eyes)\n",
    "        # Normalize VIX to 0-1 scale (approx)\n",
    "        df['VIX_Norm'] = ((macro['VIX'] - 10) / 40).clip(0, 1)\n",
    "        # Rate of Change for TNX (Speed of rate rise matters more than level)\n",
    "        df['TNX_ROC'] = macro['TNX'].pct_change(20).fillna(0) * 10 # Scale up for neural net\n",
    "        \n",
    "        # 3. Targets (Triple Barrier)\n",
    "        raw_labels = FeatureLab.triple_barrier_labels(\n",
    "            df['Adj Close'], df['Volatility'], pt=2.0, sl=2.0, barrier_window=10\n",
    "        )\n",
    "        label_map = {-1: 0, 0: 1, 1: 2} # Sell:0, Neut:1, Buy:2\n",
    "        df['Target'] = raw_labels.map(label_map)\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # --- Tensor Prep ---\n",
    "        SEQ_LEN = 30\n",
    "        feature_cols = ['FracDiff', 'Volatility', 'RSI', 'VIX_Norm', 'TNX_ROC']\n",
    "        \n",
    "        # Robust Scaler\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "        \n",
    "        data_matrix = df[feature_cols].values\n",
    "        targets = df['Target'].values\n",
    "        \n",
    "        X_seq, y_seq, indices = [], [], []\n",
    "        for i in range(len(data_matrix) - SEQ_LEN):\n",
    "            X_seq.append(data_matrix[i : i+SEQ_LEN])\n",
    "            y_seq.append(targets[i + SEQ_LEN - 1])\n",
    "            indices.append(df.index[i + SEQ_LEN - 1])\n",
    "            \n",
    "        X_tensor = torch.FloatTensor(np.array(X_seq))\n",
    "        y_tensor = torch.LongTensor(np.array(y_seq))\n",
    "        \n",
    "        # --- Continuous Learning Loop ---\n",
    "        train_window = 500\n",
    "        test_window = 21\n",
    "        signals = pd.Series(0, index=df.index)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\"))\n",
    "        print(f\"Training V15 (Macro-Aware) on {device}...\")\n",
    "        \n",
    "        # Initialize Model (Larger input_dim)\n",
    "        model = TimeSeriesTransformer(\n",
    "            input_dim=len(feature_cols), # Now 5 features\n",
    "            d_model=32, \n",
    "            nhead=2, \n",
    "            num_layers=1, \n",
    "            num_classes=3,\n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        class_weights = torch.tensor([2.0, 1.0, 2.0]).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        for t in range(train_window, len(X_tensor), test_window):\n",
    "            # Train Step\n",
    "            X_train = X_tensor[t-train_window : t].to(device)\n",
    "            y_train = y_tensor[t-train_window : t].to(device)\n",
    "            \n",
    "            # Predict Step\n",
    "            X_test = X_tensor[t : min(t+test_window, len(X_tensor))].to(device)\n",
    "            if len(X_test) == 0: break\n",
    "            \n",
    "            # Fine-Tune\n",
    "            model.train()\n",
    "            for _ in range(5): \n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_train)\n",
    "                loss = criterion(output, y_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            # Inference\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probs = model(X_test)\n",
    "                buy_probs = probs[:, 2].cpu().numpy()\n",
    "                sell_probs = probs[:, 0].cpu().numpy()\n",
    "                \n",
    "                batch_signals = np.zeros(len(X_test))\n",
    "                \n",
    "                # Higher threshold for Macro model to be \"Pickier\"\n",
    "                batch_signals[buy_probs > 0.55] = 1\n",
    "                batch_signals[sell_probs > 0.55] = 0 \n",
    "                \n",
    "                batch_idx = indices[t : min(t+test_window, len(X_tensor))]\n",
    "                signals.loc[batch_idx] = batch_signals\n",
    "\n",
    "        df['Signal'] = signals\n",
    "        \n",
    "        # Volatility Sizing\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / df['Volatility']).clip(upper=1.5)\n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec595138",
   "metadata": {},
   "source": [
    "## V21_MacroMom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10ef132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyV21_MacroMomentum(BaseStrategy):\n",
    "    \"\"\"\n",
    "    V21: The Production Candidate.\n",
    "    \n",
    "    Architecture:\n",
    "    1. BASE: V20 Macro Stress Logic (VIX + TNX).\n",
    "    2. OVERRIDE: 'Trend Floor'. If the asset is structurally bullish \n",
    "       (Price > SMA200), we refuse to go to zero exposure, even if Macro is stressed.\n",
    "       \n",
    "    This solves the 'TSLA Problem' (missing rallies) without relying on \n",
    "    unstable Neural Networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.macro_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        super().fetch_data(warmup_years)\n",
    "        \n",
    "        # 1. Fetch Macro Data\n",
    "        try:\n",
    "            start_dt = self.data.index[0]\n",
    "            vix = yf.download(\"^VIX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            tnx = yf.download(\"^TNX\", start=start_dt, end=self.end_date, progress=False, auto_adjust=True)\n",
    "            \n",
    "            if isinstance(vix.columns, pd.MultiIndex): vix.columns = vix.columns.get_level_values(0)\n",
    "            if isinstance(tnx.columns, pd.MultiIndex): tnx.columns = tnx.columns.get_level_values(0)\n",
    "            \n",
    "            self.macro_data = pd.DataFrame(index=self.data.index)\n",
    "            self.macro_data['VIX'] = vix['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "            self.macro_data['TNX'] = tnx['Close'].reindex(self.data.index).fillna(method='ffill')\n",
    "        except:\n",
    "            self.macro_data = pd.DataFrame({'VIX': 20, 'TNX': 4.0}, index=self.data.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.data is None or self.macro_data is None: return\n",
    "        df = self.data.copy()\n",
    "        macro = self.macro_data.copy()\n",
    "        \n",
    "        # --- 1. Macro Stress Engine (V20 Logic) ---\n",
    "        # VIX Stress: Normalizing 12-35 range\n",
    "        macro['VIX_Stress'] = ((macro['VIX'] - 12) / 23).clip(0, 1)\n",
    "        \n",
    "        # Rates Stress: 1-Month Rate of Change > 10% is Panic\n",
    "        macro['TNX_ROC'] = macro['TNX'].pct_change(20)\n",
    "        macro['TNX_Stress'] = (macro['TNX_ROC'] / 0.10).clip(0, 1)\n",
    "        \n",
    "        # Combined Stress (Smoothed)\n",
    "        macro['Total_Stress'] = macro[['VIX_Stress', 'TNX_Stress']].max(axis=1).rolling(5).mean().fillna(0)\n",
    "        \n",
    "        # Base Signal: Inverse of Stress\n",
    "        df['Base_Signal'] = 1.0 - macro['Total_Stress']\n",
    "        \n",
    "        # --- 2. The Momentum Override (The Fix) ---\n",
    "        # \"Don't fight the tape.\"\n",
    "        \n",
    "        # Long Term Trend (SMA 200)\n",
    "        df['SMA_200'] = df['Adj Close'].rolling(200).mean()\n",
    "        df['Trend_Bull'] = (df['Adj Close'] > df['SMA_200'])\n",
    "        \n",
    "        # Momentum (RSI)\n",
    "        df['RSI'] = FeatureLab.compute_rsi(df['Adj Close'])\n",
    "        df['Mom_Bull'] = (df['RSI'] > 50)\n",
    "        \n",
    "        # LOGIC: \n",
    "        # If Trend is BULL and Momentum is BULL, Minimum Signal = 0.5.\n",
    "        # This ensures we participate in \"Hated Rallies\" (like TSLA 2023)\n",
    "        # even if VIX is elevated.\n",
    "        \n",
    "        df['Signal'] = df['Base_Signal']\n",
    "        \n",
    "        # Apply Override\n",
    "        strong_uptrend = df['Trend_Bull'] & df['Mom_Bull']\n",
    "        df.loc[strong_uptrend, 'Signal'] = np.maximum(df.loc[strong_uptrend, 'Signal'], 0.5)\n",
    "        \n",
    "        # --- 3. Safety Veto (The \"Falling Knife\" Guard) ---\n",
    "        # If Price is crashing < Lower Bollinger, KILL the trade.\n",
    "        df['SMA_20'] = df['Adj Close'].rolling(20).mean()\n",
    "        df['BB_Lower'] = df['SMA_20'] - 2 * df['SMA_20'].rolling(20).std()\n",
    "        \n",
    "        # Allow tiny buffer (0.98)\n",
    "        is_crashing = df['Adj Close'] < (df['BB_Lower'] * 0.98)\n",
    "        df.loc[is_crashing, 'Signal'] = 0.0\n",
    "\n",
    "        # --- 4. Volatility Sizing ---\n",
    "        target_vol = 0.15 / np.sqrt(252)\n",
    "        vol_scaler = (target_vol / FeatureLab.yang_zhang_volatility(df)).clip(upper=1.5)\n",
    "        \n",
    "        df['Signal'] = df['Signal'] * vol_scaler\n",
    "        \n",
    "        self.data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e77f74",
   "metadata": {},
   "source": [
    "## The Sector Rotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d774de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSR_V1_KalmanMomentum(BaseStrategy):\n",
    "    \"\"\"\n",
    "    TSR_V1: The Sector Rotator.\n",
    "    \n",
    "    Logic:\n",
    "    1. Alpha: '12-1' Momentum Ranking (intermediate trend, no reversal noise).\n",
    "    2. Beta: Kalman Filter Dynamic Hedging (Market Neutrality).\n",
    "    3. Filter: Regime Switching based on Rate Correlation & VIX.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        # Ticker here is just a placeholder/reference (e.g., \"SECTOR_ROTATOR\")\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.sectors = ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']\n",
    "        self.sector_data = None\n",
    "        self.market_data = None\n",
    "\n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # We need a longer warmup for the 12-month momentum calculation\n",
    "        start_dt = datetime.strptime(self.start_date, \"%Y-%m-%d\") - timedelta(days=warmup_years*365)\n",
    "        start_str = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Fetch Sectors\n",
    "            print(f\"Fetching Sector Data for {self.sectors}...\")\n",
    "            sec_df = yf.download(self.sectors, start=start_str, end=self.end_date, progress=False, auto_adjust=True)['Close']\n",
    "            self.sector_data = sec_df.fillna(method='ffill')\n",
    "            \n",
    "            # 2. Fetch Macro (SPY, TLT, ^VIX)\n",
    "            print(\"Fetching Macro Data (SPY, TLT, VIX)...\")\n",
    "            macro_tickers = ['SPY', 'TLT', '^VIX']\n",
    "            macro_df = yf.download(macro_tickers, start=start_str, end=self.end_date, progress=False, auto_adjust=True)['Close']\n",
    "            self.market_data = macro_df.fillna(method='ffill')\n",
    "            \n",
    "            # Create a synthetic 'Close' for the BaseStrategy compatibility (starts at 100)\n",
    "            self.data = pd.DataFrame(index=sec_df.index)\n",
    "            self.data['Adj Close'] = 100.0 \n",
    "            self.data['Returns'] = 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"TSR Fetch Error: {e}\")\n",
    "\n",
    "    def _kalman_beta(self, market_rets, asset_rets, delta=1e-5):\n",
    "        \"\"\"\n",
    "        Estimates time-varying Beta using a Kalman Filter.\n",
    "        Observation Eq: Asset = Alpha + Beta * Market\n",
    "        State Eq: Beta follows Random Walk.\n",
    "        \"\"\"\n",
    "        # Efficient numpy implementation\n",
    "        n = len(market_rets)\n",
    "        beta = np.zeros(n)\n",
    "        alpha = np.zeros(n)\n",
    "        \n",
    "        # Initial State\n",
    "        P = np.eye(2) # Covariance\n",
    "        theta = np.zeros(2) # [Alpha, Beta]\n",
    "        \n",
    "        # Transpose matrices for speed\n",
    "        y = asset_rets.values\n",
    "        x = market_rets.values\n",
    "        \n",
    "        # Loop (Kalman is recursive, hard to vectorize fully without library)\n",
    "        # We use a simplified RLS-like update for speed in backtest\n",
    "        # Q (Process Variance) determines adaptivity\n",
    "        Q = np.eye(2) * delta \n",
    "        R = 0.001 # Measurement Variance\n",
    "        \n",
    "        for t in range(n):\n",
    "            if np.isnan(y[t]) or np.isnan(x[t]): continue\n",
    "            \n",
    "            # Observation Matrix H = [1, Market_t]\n",
    "            H = np.array([1.0, x[t]])\n",
    "            \n",
    "            # Predict\n",
    "            # theta_pred = theta (Random Walk)\n",
    "            P_pred = P + Q\n",
    "            \n",
    "            # Update\n",
    "            error = y[t] - np.dot(H, theta)\n",
    "            S = np.dot(H, np.dot(P_pred, H.T)) + R\n",
    "            K = np.dot(P_pred, H.T) / S\n",
    "            \n",
    "            theta = theta + K * error\n",
    "            P = np.eye(2) - np.outer(K, H)\n",
    "            P = np.dot(P, P_pred)\n",
    "            \n",
    "            beta[t] = theta[1]\n",
    "            alpha[t] = theta[0]\n",
    "            \n",
    "        return pd.Series(beta, index=market_rets.index)\n",
    "\n",
    "    def generate_signals(self):\n",
    "        if self.sector_data is None or self.market_data is None: return\n",
    "        \n",
    "        # --- 1. Momentum Calculation (12-1) ---\n",
    "        # \"12-1\" means Return from t-12 months to t-1 month. [cite: 541]\n",
    "        # Approx 252 trading days. t-1 month is approx 21 days.\n",
    "        # Formula: P(t-21) / P(t-252) - 1\n",
    "        mom_12_1 = self.sector_data.shift(21) / self.sector_data.shift(252) - 1.0\n",
    "        \n",
    "        # --- 2. Regime Identification ---\n",
    "        spy = self.market_data['SPY']\n",
    "        tlt = self.market_data['TLT']\n",
    "        vix = self.market_data['^VIX']\n",
    "        \n",
    "        # Trend Filter (SPY > 200 SMA) [cite: 647]\n",
    "        spy_trend = (spy > spy.rolling(200).mean())\n",
    "        \n",
    "        # Inflation/Correlation Filter [cite: 651]\n",
    "        # Correlation between Stocks (SPY) and Bonds (TLT)\n",
    "        corr_spy_tlt = spy.pct_change().rolling(60).corr(tlt.pct_change())\n",
    "        \n",
    "        # VIX Stress [cite: 658]\n",
    "        vix_stress = (vix > 40)\n",
    "        \n",
    "        # --- 3. Portfolio Construction Loop ---\n",
    "        # We simulate the daily holdings\n",
    "        \n",
    "        portfolio_curve = [100.0]\n",
    "        cash = 100.0\n",
    "        \n",
    "        # Align data\n",
    "        rets = self.sector_data.pct_change().fillna(0)\n",
    "        spy_rets = spy.pct_change().fillna(0)\n",
    "        \n",
    "        # Pre-calculate Betas for all sectors (Expensive but necessary)\n",
    "        print(\"Calculating Kalman Betas for all sectors...\")\n",
    "        sector_betas = pd.DataFrame(index=rets.index, columns=rets.columns)\n",
    "        for col in rets.columns:\n",
    "            sector_betas[col] = self._kalman_beta(spy_rets, rets[col])\n",
    "            \n",
    "        # Rebalance Monthly (every 21 days) or Daily? \n",
    "        # Paper implies Monthly ranking, but daily hedging. We'll rank daily for smoothness.\n",
    "        \n",
    "        # Logic vectors\n",
    "        long_mask = pd.DataFrame(False, index=mom_12_1.index, columns=mom_12_1.columns)\n",
    "        short_mask = pd.DataFrame(False, index=mom_12_1.index, columns=mom_12_1.columns)\n",
    "        \n",
    "        # Ranking\n",
    "        ranks = mom_12_1.rank(axis=1, ascending=False)\n",
    "        # Top 3 Long, Bottom 3 Short\n",
    "        for t in range(len(ranks)):\n",
    "            # Determine Regime\n",
    "            date = ranks.index[t]\n",
    "            is_bull = spy_trend.iloc[t]\n",
    "            corr = corr_spy_tlt.iloc[t]\n",
    "            is_panic = vix_stress.iloc[t]\n",
    "            \n",
    "            # DEFAULT: Long Top 3, Short Bottom 3\n",
    "            # REGIME OVERRIDES [cite: 660]\n",
    "            \n",
    "            if is_panic:\n",
    "                # Crash: Cash\n",
    "                pass \n",
    "            elif not is_bull:\n",
    "                # Bear Market\n",
    "                if corr > 0.2:\n",
    "                    # Inflation Shock (Stocks & Bonds down): Cash\n",
    "                    pass\n",
    "                elif corr < -0.2:\n",
    "                    # Growth Shock (Bonds hedge Stocks): Long Defensives (XLU, XLP)\n",
    "                    # We override the ranking to force XLU/XLP\n",
    "                    # (Simplified: just hold XLU/XLP equal weight)\n",
    "                    # We handle this in the returns loop below\n",
    "                    pass\n",
    "                else:\n",
    "                    # Normal Bear: Market Neutral (Long Winners / Short Losers)\n",
    "                    pass\n",
    "            else:\n",
    "                # Bull Market: Standard Momentum\n",
    "                pass\n",
    "                \n",
    "        # --- Fast Vectorized Backtest ---\n",
    "        # Selecting Top 3 and Bottom 3\n",
    "        top_3 = (ranks <= 3)\n",
    "        bot_3 = (ranks >= 7) # 9 sectors total, 7,8,9 are bottom\n",
    "        \n",
    "        # 1. Calculate Basket Returns\n",
    "        long_basket_ret = (rets * top_3).sum(axis=1) / 3\n",
    "        short_basket_ret = (rets * bot_3).sum(axis=1) / 3\n",
    "        \n",
    "        # 2. Calculate Basket Betas\n",
    "        long_basket_beta = (sector_betas * top_3).sum(axis=1) / 3\n",
    "        short_basket_beta = (sector_betas * bot_3).sum(axis=1) / 3\n",
    "        \n",
    "        # 3. Calculate Hedge Ratio [cite: 612]\n",
    "        # H = Beta_L / Beta_S\n",
    "        # Avoid division by zero\n",
    "        hedge_ratio = (long_basket_beta / short_basket_beta.replace(0, 1)).clip(0, 2)\n",
    "        \n",
    "        # 4. Construct Strategy Return Stream\n",
    "        # Standard: Beta Neutral\n",
    "        # Weights: w_L = 1 / (1+H), w_S = H / (1+H) [cite: 618]\n",
    "        w_L = 1 / (1 + hedge_ratio)\n",
    "        w_S = hedge_ratio / (1 + hedge_ratio)\n",
    "        \n",
    "        strat_ret = (w_L * long_basket_ret) - (w_S * short_basket_ret)\n",
    "        \n",
    "        # 5. Apply Regime Overrides (Vectorized)\n",
    "        \n",
    "        # Condition A: Crash (VIX > 40) -> Return = 0 (Cash) [cite: 658]\n",
    "        strat_ret[vix_stress] = 0.0\n",
    "        \n",
    "        # Condition B: Bear + Inflation (Corr > 0.2) -> Return = 0 (Cash) [cite: 656]\n",
    "        bear_inflation = (~spy_trend) & (corr_spy_tlt > 0.2)\n",
    "        strat_ret[bear_inflation] = 0.0\n",
    "        \n",
    "        # Condition C: Bear + Growth Shock (Corr < -0.2) -> Long Defensives [cite: 653]\n",
    "        # Here we switch from L/S to Long Only XLU+XLP\n",
    "        bear_growth = (~spy_trend) & (corr_spy_tlt < -0.2)\n",
    "        defensive_ret = (rets['XLU'] + rets['XLP']) / 2\n",
    "        strat_ret[bear_growth] = defensive_ret[bear_growth]\n",
    "        \n",
    "        # Accumulate\n",
    "        self.data['Returns'] = strat_ret\n",
    "        self.data['Adj Close'] = (1 + strat_ret).cumprod() * 100.0\n",
    "        self.data['Signal'] = 1 # Dummy signal to satisfy BaseStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a6be1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy_Ensemble_HillClimb(BaseStrategy):\n",
    "    \"\"\"\n",
    "    Fixed Ensemble: Correctly handles internal backtesting to populate \n",
    "    performance metrics for Hill Climbing optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        super().__init__(ticker, start_date, end_date)\n",
    "        self.v21 = StrategyV21_MacroMomentum(ticker, start_date, end_date)\n",
    "        self.tsr = TSR_V1_KalmanMomentum(\"SECTOR_ALPHA\", start_date, end_date)\n",
    "        \n",
    "    def fetch_data(self, warmup_years=2):\n",
    "        # 1. Fetch and Generate for Sub-Strategies\n",
    "        self.v21.fetch_data(warmup_years)\n",
    "        self.v21.generate_signals()\n",
    "        # CRITICAL: Populate 'Net_Returns' via backtest\n",
    "        self.v21.run_backtest() \n",
    "        \n",
    "        self.tsr.fetch_data(warmup_years)\n",
    "        self.tsr.generate_signals()\n",
    "        # Populates TSR returns (assuming TSR uses Net_Returns in ensemble)\n",
    "        self.tsr.run_backtest() \n",
    "\n",
    "        # 2. Align Data\n",
    "        self.data = pd.DataFrame(index=self.v21.data.index)\n",
    "        # Use 'Net_Returns' for the optimization to account for costs\n",
    "        self.data['Ret_V21'] = self.v21.results['Net_Returns']\n",
    "        self.data['Ret_TSR'] = self.tsr.results['Net_Returns']\n",
    "        self.data.dropna(inplace=True)\n",
    "        \n",
    "    def generate_signals(self):\n",
    "        # Parameters for the Hill Climbing search [cite: 301]\n",
    "        lookback = 126\n",
    "        rebalance_step = 21 \n",
    "        \n",
    "        rets = self.data[['Ret_V21', 'Ret_TSR']].values\n",
    "        n = len(rets)\n",
    "        \n",
    "        # Initial weight (50/50)\n",
    "        best_w = 0.5\n",
    "        weights_v21 = np.zeros(n)\n",
    "        \n",
    "        for t in range(lookback, n, rebalance_step):\n",
    "            window = rets[t-lookback : t]\n",
    "            \n",
    "            # Hill Climbing: Test current, +5%, -5%\n",
    "            candidates = [best_w, np.clip(best_w + 0.05, 0, 1), np.clip(best_w - 0.05, 0, 1)]\n",
    "            \n",
    "            best_sharpe = -np.inf\n",
    "            for w in candidates:\n",
    "                # Combine returns based on candidate weight\n",
    "                port_ret = w * window[:, 0] + (1 - w) * window[:, 1]\n",
    "                sharpe = np.mean(port_ret) / (np.std(port_ret) + 1e-9)\n",
    "                \n",
    "                if sharpe > best_sharpe:\n",
    "                    best_sharpe = sharpe\n",
    "                    best_w = w\n",
    "            \n",
    "            # Apply optimized weight to the next month\n",
    "            end_idx = min(t + rebalance_step, n)\n",
    "            weights_v21[t:end_idx] = best_w\n",
    "            \n",
    "        self.data['W_V21'] = weights_v21\n",
    "        self.data['Returns'] = (self.data['Ret_V21'] * self.data['W_V21']) + \\\n",
    "                               (self.data['Ret_TSR'] * (1 - self.data['W_V21']))\n",
    "        \n",
    "        self.data['Adj Close'] = (1 + self.data['Returns']).cumprod() * 100\n",
    "        self.data['Signal'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6ae43",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c02190db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustBenchmark:\n",
    "    \"\"\"\n",
    "    Implements Walk-Forward Analysis and Deflated Sharpe Ratio logic.\n",
    "    Benchmarks multiple strategies without look-ahead bias[cite: 275].\n",
    "    \"\"\"\n",
    "    def __init__(self, tickers, start_date, end_date):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.results = []\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"{'STRATEGY':<10} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        strategies = {\n",
    "            \"V1_Base\": StrategyV1_Baseline,\n",
    "        }\n",
    "\n",
    "        for ticker in self.tickers:\n",
    "            # Capture Buy & Hold first\n",
    "            bh = StrategyV1_Baseline(ticker, self.start_date, self.end_date)\n",
    "            bh.fetch_data()\n",
    "            bh.data['Signal'] = 1 # Force Buy\n",
    "            bh.run_backtest()\n",
    "            self._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "            \n",
    "            for name, StratClass in strategies.items():\n",
    "                try:\n",
    "                    strat = StratClass(ticker, self.start_date, self.end_date)\n",
    "                    strat.fetch_data(warmup_years=2)\n",
    "                    strat.generate_signals()\n",
    "                    strat.run_backtest()\n",
    "                    \n",
    "                    self._print_row(name, ticker, strat.metrics)\n",
    "                    \n",
    "                    # Store for portfolio level (optional)\n",
    "                    self.results.append({\n",
    "                        'Ticker': ticker,\n",
    "                        'Strategy': name,\n",
    "                        'Returns': strat.results['Net_Returns']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed {name} {ticker}: {e}\")\n",
    "            print(\"-\" * 75)\n",
    "\n",
    "    def _print_row(self, name, ticker, metrics):\n",
    "        if not metrics: return\n",
    "        ret = metrics['Total Return']\n",
    "        # Annualize return approx\n",
    "        ann_ret = (1 + ret) ** (252 / len(metrics.get('Returns', [1]*252))) - 1 if 'Returns' in metrics else ret\n",
    "        print(f\"{name:<10} | {ticker:<6} | {ret:.1%}   | {metrics['Sharpe Ratio']:.2f}   | {metrics['Max Drawdown']:.1%}   |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac5dc656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRATEGY     | TICKER | ANN RET | SHARPE | MAX DD  | NOTES\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | NVDA   | 355.4%   | 1.19   | -62.7%   |\n",
      "V12_Macro  | NVDA   | 159.6%   | 1.57   | -16.7%   |\n",
      "V21_MacroMom | NVDA   | 95.6%   | 1.60   | -12.0%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "TSR_V1     | NVDA   | 1.3%   | 0.10   | -11.1%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "ENS_HillClimb | NVDA   | 28.6%   | 0.94   | -8.4%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | JPM    | 62.1%   | 0.77   | -37.9%   |\n",
      "V12_Macro  | JPM    | 93.1%   | 1.25   | -13.7%   |\n",
      "V21_MacroMom | JPM    | 59.0%   | 1.25   | -12.3%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "TSR_V1     | JPM    | 1.3%   | 0.10   | -11.1%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "ENS_HillClimb | JPM    | 23.2%   | 0.79   | -9.4%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | TSLA   | 7.9%   | 0.35   | -73.0%   |\n",
      "V12_Macro  | TSLA   | 30.8%   | 0.58   | -25.0%   |\n",
      "V21_MacroMom | TSLA   | 31.2%   | 0.78   | -11.3%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "TSR_V1     | TSLA   | 1.3%   | 0.10   | -11.1%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "ENS_HillClimb | TSLA   | 19.5%   | 0.69   | -9.2%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | BABA   | -26.9%   | 0.06   | -54.0%   |\n",
      "V12_Macro  | BABA   | 0.4%   | 0.08   | -22.0%   |\n",
      "V21_MacroMom | BABA   | 3.0%   | 0.14   | -15.9%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "TSR_V1     | BABA   | 1.3%   | 0.10   | -11.1%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "ENS_HillClimb | BABA   | -3.3%   | -0.13   | -13.5%   |\n",
      "-------------------------------------------------------------------------------\n",
      "Buy&Hold   | XLE    | 65.0%   | 0.77   | -26.0%   |\n",
      "V12_Macro  | XLE    | 4.5%   | 0.17   | -24.4%   |\n",
      "V21_MacroMom | XLE    | 13.5%   | 0.39   | -13.7%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "TSR_V1     | XLE    | 1.3%   | 0.10   | -11.1%   |\n",
      "Fetching Sector Data for ['XLE', 'XLF', 'XLK', 'XLY', 'XLI', 'XLV', 'XLP', 'XLU', 'XLB']...\n",
      "Fetching Macro Data (SPY, TLT, VIX)...\n",
      "Calculating Kalman Betas for all sectors...\n",
      "ENS_HillClimb | XLE    | -1.1%   | 0.02   | -11.6%   |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"{'STRATEGY':<12} | {'TICKER':<6} | {'ANN RET':<7} | {'SHARPE':<6} | {'MAX DD':<7} | {'NOTES'}\")\n",
    "    print(\"-\" * 79)\n",
    "    \n",
    "    # Helper wrappers for the dictionary\n",
    "    class Strategy_Ensemble_5050(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.5, 0.5)\n",
    "\n",
    "    class Strategy_Ensemble_Growth(Strategy_Ensemble):\n",
    "        def __init__(self, ticker, start, end): super().__init__(ticker, start, end, 0.7, 0.3)\n",
    "\n",
    "    strategies = {\n",
    "        # \"V3_Macro\": StrategyV3_Macro,\n",
    "        # \"V9_Unshack\": StrategyV9_RegimeUnshackled,\n",
    "        # \"Ens_Bal\": Strategy_Ensemble_5050,      # Static 50/50\n",
    "        # \"Ens_Grow\": Strategy_Ensemble_Growth,   # Static 70/30\n",
    "        # \"Ens_Adapt\": Strategy_Ensemble_Adaptive, # Dynamic V10\n",
    "        # \"HRP_Base\": Strategy_Ensemble_HRP,\n",
    "        \"V12_Macro\": StrategyV12_Macro_Switch,\n",
    "        \"V21_MacroMom\": StrategyV21_MacroMomentum,\n",
    "        \"TSR_V1\": TSR_V1_KalmanMomentum,             # New Component\n",
    "        \"ENS_HillClimb\": Strategy_Ensemble_HillClimb # The Final Product\n",
    "    }\n",
    "\n",
    "    # Same Stress Test Basket\n",
    "    tickers = [\"NVDA\", \"JPM\", \"TSLA\", \"BABA\", \"XLE\"]\n",
    "\n",
    "    bench = RobustBenchmark(\n",
    "        tickers=tickers, \n",
    "        start_date=\"2022-01-01\", \n",
    "        end_date=\"2024-12-30\"\n",
    "    )\n",
    "    \n",
    "    # Manual run loop to handle the specific classes\n",
    "    for ticker in tickers:\n",
    "        # Buy & Hold\n",
    "        bh = StrategyV1_Baseline(ticker, bench.start_date, bench.end_date)\n",
    "        bh.fetch_data()\n",
    "        bh.data['Signal'] = 1\n",
    "        bh.run_backtest()\n",
    "        bench._print_row(\"Buy&Hold\", ticker, bh.metrics)\n",
    "        \n",
    "        for name, StratClass in strategies.items():\n",
    "            try:\n",
    "                strat = StratClass(ticker, bench.start_date, bench.end_date)\n",
    "                strat.fetch_data(warmup_years=2)\n",
    "                strat.generate_signals()\n",
    "                strat.run_backtest()\n",
    "                bench._print_row(name, ticker, strat.metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Err {name} {ticker}: {e}\")\n",
    "        print(\"-\" * 79)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
